<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/narsil.github.io/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Self KL-divergence for detecting out of distribution data and unsupervised text classification | Narsil</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Self KL-divergence for detecting out of distribution data and unsupervised text classification" />
<meta name="author" content="nicolas" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="TL;DR. By training two models at the same time (same architecture, same loss, but different initialization) I was able to obtain a consistent out-of-distribution detector by measuring the kl-divergence between model outputs. This out-of-distribution measure used on text could lead to unsupervised text classification." />
<meta property="og:description" content="TL;DR. By training two models at the same time (same architecture, same loss, but different initialization) I was able to obtain a consistent out-of-distribution detector by measuring the kl-divergence between model outputs. This out-of-distribution measure used on text could lead to unsupervised text classification." />
<link rel="canonical" href="https://narsil.github.io/narsil.github.io/2020/02/26/self-kl-divergence-for-out-of-distribution-detection.html" />
<meta property="og:url" content="https://narsil.github.io/narsil.github.io/2020/02/26/self-kl-divergence-for-out-of-distribution-detection.html" />
<meta property="og:site_name" content="Narsil" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-26T00:00:00-06:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"nicolas"},"description":"TL;DR. By training two models at the same time (same architecture, same loss, but different initialization) I was able to obtain a consistent out-of-distribution detector by measuring the kl-divergence between model outputs. This out-of-distribution measure used on text could lead to unsupervised text classification.","@type":"BlogPosting","headline":"Self KL-divergence for detecting out of distribution data and unsupervised text classification","dateModified":"2020-02-26T00:00:00-06:00","url":"https://narsil.github.io/narsil.github.io/2020/02/26/self-kl-divergence-for-out-of-distribution-detection.html","datePublished":"2020-02-26T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://narsil.github.io/narsil.github.io/2020/02/26/self-kl-divergence-for-out-of-distribution-detection.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/narsil.github.io/assets/main.css">
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://narsil.github.io/narsil.github.io/feed.xml" title="Narsil" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>
</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/narsil.github.io/">Narsil</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/narsil.github.io/about/">About Me</a><a class="page-link" href="/narsil.github.io/search/">Search</a><a class="page-link" href="/narsil.github.io/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Self KL-divergence for detecting out of distribution data and unsupervised text classification</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-02-26T00:00:00-06:00" itemprop="datePublished">
        Feb 26, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">nicolas</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      2 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <blockquote>
  <p>TL;DR. By training two models at the same time (same architecture, same loss, but different initialization)
I was able to obtain a consistent out-of-distribution detector by measuring the kl-divergence between model outputs.
This out-of-distribution measure used on text could lead to unsupervised text classification.</p>
</blockquote>

<p>1/ What’s the problem ?</p>

<p>ML models usually are not really capable of predicting how well the data you
feed them is close to what was in the dataset. It really matters in production
models as they might make really stupid mistakes just because they are off
the training set.</p>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Find a good example of failure</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Find classical solutions to this problem.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Good thing that makes this measure better than other methods is that it’s measured in bits/nats so you know.</li>
  <li class="task-list-item">
    <p><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Less tl;dr as to why this should work (lottery ticket hypothesis, model structure forces some relevant manifold of the output</p>
  </li>
  <li class="task-list-item">https://ai.googleblog.com/2019/12/improving-out-of-distribution-detection.html</li>
  <li class="task-list-item">https://arxiv.org/pdf/1910.04241.pdf (Manifold approximation via Embedding (VAE or GAN) and out-of-distribution sampling via Manifold perturbation.)</li>
  <li class="task-list-item">https://paperswithcode.com/task/out-of-distribution-detection</li>
  <li class="task-list-item">https://openreview.net/pdf?id=Hkxzx0NtDB (Energy based model, hard to train but effective, learn <code class="language-plaintext highlighter-rouge">p(x, y)</code> at the same time as <code class="language-plaintext highlighter-rouge">p(y|x)</code>)</li>
  <li class="task-list-item">https://arxiv.org/pdf/1802.04865v1.pdf (Adding extra loss making optimization problem joint between confidence and accuracy)</li>
</ul>

<p>2/ How do we solve it ?</p>

<p>Tl;dr : Make two similar models, with two different random initialization, then train them at the same time.
Check their converged average kl-distance on the train set, that will give you a baseline of what similar is.</p>

<p>Check what kind of values do you get on test/validation set. You should get something similar or higher.</p>

<p>Then you have by measuring this self kl-divergence on new sample a measure of newness. Then it’s a matter of choosing your
own threshold about what’s acceptable or not.</p>

<p>linked to the training data leading to good properties in terms of out of distribution values).</p>

<p>3/ Experiments</p>

<ul>
  <li>Test two identical networks. With same training we should have kl-divergence = 0 everywhere. So no possibility of detecting out of distribution.
Test on widely different architecture and check that we don’t get correct results</li>
  <li>On same architecture, different initialization show that it can be used for out of distribution detection for english, french and the train set.</li>
  <li>Test with various initialization patterns, with various architectures. Show that it’s linked to
descent method, and probably structure of network (does not seem fully generalizable)</li>
  <li>Test with random inputs to check that it works.</li>
  <li>Test with adversarial sampling to see if we can generate samples from it.</li>
</ul>

<p>4/ Unsupervised text classification</p>

<ul>
  <li>Show that small network trained on a single english book enables to detect different languages
or different patterns of writing (old english, irish, french, or event dictionnaries)</li>
  <li>The detection is super fined grained capable of detecting english within a French book.</li>
</ul>

<p>5/ Limits</p>

<p>6/ Future work</p>

  </div><a class="u-url" href="/narsil.github.io/2020/02/26/self-kl-divergence-for-out-of-distribution-detection.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/narsil.github.io/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Narsil</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Narsil</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/Narsil"><svg class="social svg-icon"><use xlink:href="/narsil.github.io/assets/minima-social-icons.svg#github"></use></svg> <span class="username">Narsil</span></a></li><li><a href="https://www.twitter.com/narsilou"><svg class="social svg-icon"><use xlink:href="/narsil.github.io/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">narsilou</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Small experiements insights from ML and software development.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
