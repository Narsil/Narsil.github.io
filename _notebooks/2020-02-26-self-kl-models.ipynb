{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self KL-divergence for detecting out of distribution data and unsupervised text classification\n",
    "> Running two models alongside each other for trivial out of distribution detection in production models and side bonus is getting unsupervised text classification\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [ml, nlp, kldivergence]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TL;DR. By training two models at the same time (same architecture, same loss, but different initialization)\n",
    "> I was able to obtain a consistent out-of-distribution detector by measuring the kl-divergence between model outputs.\n",
    "> This out-of-distribution measure used on text could lead to unsupervised text classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the problem ?\n",
    "\n",
    "ML models usually are not really capable of predicting how well the data you     \n",
    "feed them is close to what was in the dataset. It really matters in production \n",
    "models as they might make really stupid mistakes just because they are off       \n",
    "the training set.                             \n",
    "\n",
    "\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a simple mnist model (straight out from pytorch tutorial https://github.com/pytorch/examples/tree/master/mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/site-packages (1.4.0)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/site-packages (0.5.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from torchvision) (1.18.1)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from torchvision) (1.14.0)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/site-packages (from torchvision) (7.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def mnist():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "\n",
    "    parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(os.path.expanduser('../data'), train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "        \n",
    "# mnist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.296684\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.748730\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.663051\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.494142\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.438472\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.384286\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.385418\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.250876\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.703926\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.355346\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.278224\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.331458\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.298988\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.209039\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.170960\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.115530\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.111752\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.204197\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.189492\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.153070\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.228078\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.112070\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.212353\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.093582\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.172110\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.110639\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.061271\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.057330\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.198386\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.113547\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.171245\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.227756\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.157827\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.167113\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.184704\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.146652\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.106704\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.038050\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.130774\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.042801\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.091517\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.170126\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.240044\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.059765\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.225618\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.048066\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.161970\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.125340\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.317494\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.039830\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.156603\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.021592\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.105010\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.036694\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.171720\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.093361\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.085740\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.254587\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.219403\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.032687\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.252310\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.165599\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.049165\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.180152\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.109301\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.131337\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.176562\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.252798\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.131634\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.148465\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.067477\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.039815\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.028694\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.224834\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.026540\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.111858\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.100877\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.185940\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.135178\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.021596\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.197833\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.073312\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.051910\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.237681\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.259441\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.086435\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.217437\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.075734\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.029178\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.144421\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.095352\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.090202\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.034752\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.137363\n",
      "\n",
      "Test set: Average loss: 0.0623, Accuracy: 9787/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.067744\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.146192\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.108965\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.154003\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.116654\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.044878\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.018982\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.028592\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.075553\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.106339\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.059158\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.035717\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.050670\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.083442\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.175607\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.058346\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.096284\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.101970\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.078326\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.018900\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.072181\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.080082\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.041451\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.067524\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.061077\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.051676\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.013357\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.032548\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.034441\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.013410\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.151495\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.013085\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.023814\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.023893\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.078105\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.141561\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.017410\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.048207\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.192905\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.103398\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.049071\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.037532\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.042636\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.048587\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.034824\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.074354\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.068479\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.135081\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.013664\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.036536\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.115836\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.060726\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.019435\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.149379\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.012304\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.203113\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.287232\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.062847\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.090300\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.116553\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.034506\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.069434\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.027978\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.061708\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.076393\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.064283\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.045569\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.060856\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.035931\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.006007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.013835\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.066317\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.073026\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.013401\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.019004\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.121260\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.006645\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.091914\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.073917\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.023371\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.115739\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.219041\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.017631\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.094882\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.110504\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.048632\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.091261\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.007711\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.191920\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.039812\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.031338\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.030448\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.089500\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.022688\n",
      "\n",
      "Test set: Average loss: 0.0390, Accuracy: 9872/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.004564\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.066707\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.042990\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.042087\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.026547\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.111960\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.061295\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.137335\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.099568\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.119029\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.009102\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.026470\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.058063\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.024402\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.099415\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.073575\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.050796\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.219859\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.030950\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.078243\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.241837\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.033650\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.003147\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.101811\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.070171\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.108155\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.003658\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.055546\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.049507\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.153781\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.044520\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.004762\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.068127\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.014001\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.106911\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.032491\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.012718\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.004378\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.013831\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.077801\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.057599\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.031834\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.027212\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.014877\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.161193\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.125008\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.054370\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.220758\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.117301\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.009563\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.007389\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.047435\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.034377\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.027233\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.060727\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.058760\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.013805\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.027461\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.029963\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.120638\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.031825\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.134270\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.082666\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.021492\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.101200\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.047373\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.040572\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.071791\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.106522\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.056855\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.114074\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.021627\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.024919\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.018301\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.084489\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.016685\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.005875\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.082374\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.049157\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.051516\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.095321\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.038174\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.150737\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.079810\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.085997\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.100473\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.063647\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.029732\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.032795\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.018285\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.002732\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.035579\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.010209\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.032909\n",
      "\n",
      "Test set: Average loss: 0.0377, Accuracy: 9884/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.012807\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.060787\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.011989\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.063168\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.018507\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.009307\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.041096\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.047690\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.131630\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.007220\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.028600\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.016547\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.052478\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.044815\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.019904\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.052997\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.106422\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.002547\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.023910\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.018809\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.012277\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.024832\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.084127\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.063277\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.005836\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.018697\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.089421\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.002055\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.045900\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.183816\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.003963\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.004794\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.023431\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.058132\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.068739\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.005740\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.043515\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.047052\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.016577\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.001230\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.093007\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.002134\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.089486\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.024755\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.198825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.018943\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.000792\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.145806\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.103537\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.015214\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.021014\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.014712\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.025855\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.005132\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.007938\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.008412\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.006710\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.210589\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.036074\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.037000\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.033628\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.017531\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.004076\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.129550\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.020155\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.059665\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.011738\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.005694\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.138651\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.003600\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.006774\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.101004\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.043139\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.147411\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.033572\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.014055\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.013240\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.003886\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.055537\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.097916\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.013632\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.008840\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.048117\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.084814\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.011351\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.007341\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.071645\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.009212\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.020403\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.005681\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.054166\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.129404\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.113054\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.007507\n",
      "\n",
      "Test set: Average loss: 0.0334, Accuracy: 9896/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.072369\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.005570\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.006210\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.071049\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.017997\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.065829\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.061996\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.017837\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.064963\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.029309\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.001004\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.010038\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.019683\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.005836\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.014261\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.005943\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.001390\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.009475\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.018961\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.034109\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.168422\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.010206\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.055711\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.043667\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.015244\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.051842\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.009078\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.101767\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.050613\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.004948\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.031991\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.004457\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.006105\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.011266\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.014909\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.011884\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.016720\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.001895\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.015710\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.006051\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.042448\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.002874\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.021522\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.022087\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.008655\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.120484\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.000809\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.009889\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.003367\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.223241\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.053997\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.007883\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.118654\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.101968\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.005312\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.002986\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.009090\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.099430\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.006137\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.002585\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.000857\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.001176\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.005920\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.040936\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.024170\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.026588\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.091715\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.014411\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.016765\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.001699\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.193626\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.010892\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.070181\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.037401\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.078660\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.001244\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.021096\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.004610\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.020471\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.016372\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.010670\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.006278\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.045114\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.002892\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.008894\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.009015\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.017871\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.023274\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.005530\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.046375\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.010310\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.047666\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.006099\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.012828\n",
      "\n",
      "Test set: Average loss: 0.0352, Accuracy: 9898/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.003070\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.055580\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.003293\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.024554\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.000216\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.008619\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.001720\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.115799\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.000883\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.003905\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.107041\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.122019\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.033569\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.015421\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.027175\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.005273\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.117649\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.010915\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.013495\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.050102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.002318\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.040155\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.013842\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.002330\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.024685\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.003700\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.021113\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.057999\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.011047\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.067166\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.001192\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.000563\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.010040\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.007561\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.032846\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.013236\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.005575\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.000248\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.013824\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.002829\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.006688\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.018076\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.018678\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.047318\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.005305\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.002031\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.009330\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.124835\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.014918\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.054121\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.031004\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.016806\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.025367\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.092176\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.003536\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.010658\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.016895\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.003105\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.020862\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.017400\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.002735\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.066924\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.029526\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.046981\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.085276\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.081656\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.008673\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.005668\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.005546\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.001908\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.027402\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.017392\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.023060\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.001214\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.008123\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.199912\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.144467\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.046780\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.023137\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.039486\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.002578\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.112551\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.104812\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.005276\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.001370\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.004770\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.010748\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.027303\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.005983\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.010090\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.004167\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.011933\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.009326\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.029760\n",
      "\n",
      "Test set: Average loss: 0.0292, Accuracy: 9916/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.011360\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.055608\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.006625\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.058168\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.023565\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.001618\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.054487\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.006560\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.011615\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.012427\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.018340\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.000541\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.124543\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.014927\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.029073\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.111035\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.040552\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.002215\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.108794\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.114098\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.118534\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.172338\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.002050\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.000394\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.000396\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.000539\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.005360\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.127712\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.024948\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.001182\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.010058\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.102667\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.053093\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.004336\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.011861\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.054160\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.014249\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.006841\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.001913\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.006228\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.036544\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.031009\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.004895\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.002105\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.005571\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.002962\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.001352\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.141055\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.002534\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.084504\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.018159\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.003553\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.003380\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.010571\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.004332\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.006332\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.028123\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.013200\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.047094\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.003724\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.162278\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.009259\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.017890\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.019436\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.001533\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.019331\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.002624\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.116388\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.027720\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.061739\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.007096\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.042334\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.008200\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.052945\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.079092\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.037989\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.007254\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.004145\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.010487\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.001930\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.033197\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.031000\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.035886\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.003008\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.007244\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.002939\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.006655\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.003528\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.019807\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.014724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.023662\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.002546\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.003831\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.100814\n",
      "\n",
      "Test set: Average loss: 0.0306, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.058938\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.073502\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.011661\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.088264\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.001176\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.003380\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.000436\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.007919\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.096891\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.130878\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.039486\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.006028\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.007877\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.055661\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.003264\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.031046\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.002693\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.033745\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.010213\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.020679\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.011602\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.008249\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.007817\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.009509\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.009345\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.025606\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.105027\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.018599\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.008897\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.035350\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.004504\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.013051\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.003054\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.000735\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.003353\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.018716\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.038733\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.007646\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.008405\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.131993\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.132188\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.000087\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.000824\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.002287\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.040125\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.018559\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.003228\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.006823\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.018409\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.002721\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006241\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.032375\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.023630\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.019648\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.035156\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.003563\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.002710\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.135224\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.008372\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.042086\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.025411\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.001042\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.008341\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.016421\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.062886\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.013329\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.001042\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.000322\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.001629\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.001194\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.002600\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.055484\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.150192\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.026498\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.016883\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.012440\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.003707\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.017644\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.006424\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.003912\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.007140\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.008781\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.042857\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.007232\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.003171\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.000684\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.034667\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.005646\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.001423\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.031753\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.000269\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.091212\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.199095\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.004874\n",
      "\n",
      "Test set: Average loss: 0.0309, Accuracy: 9907/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.013658\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.000932\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.002659\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.034181\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.022893\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.016698\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.001195\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.014750\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.010572\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.000431\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.028926\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.003342\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.086724\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.007604\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.000700\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.002911\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.001704\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.004882\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.015385\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.000999\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.141458\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.006191\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.004278\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.010720\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.131137\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.000329\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.035724\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.005080\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.025359\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.056199\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.002677\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.001932\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.000284\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.114408\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.001022\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.000784\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.023267\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.034823\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.038982\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.002647\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.008841\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.065644\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.016144\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.000256\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.001198\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.006815\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.024937\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.001458\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.011804\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.005028\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.021888\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.025729\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.002463\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.006903\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.020523\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.045818\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.004742\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.042214\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.028129\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.003373\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.022877\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.001046\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.028627\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.001003\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.012343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.008067\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.000237\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.085330\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.006433\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.008115\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.053890\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.029573\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.032294\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.002944\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.044455\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.002994\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.002355\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.005650\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.002886\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.024059\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.011037\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.031738\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.007290\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.001360\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.027384\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.032603\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.106180\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.090485\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.042541\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.003144\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.016118\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.029894\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.018309\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.001471\n",
      "\n",
      "Test set: Average loss: 0.0307, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001025\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.001629\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.001575\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.017497\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.007125\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.012167\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.000740\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.023642\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.002276\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.008502\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.005546\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.021516\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.009040\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.014851\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.005159\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.018941\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.001369\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.014010\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.001135\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.001870\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.041816\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.209208\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.006623\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.012840\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.044105\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.006200\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.063212\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.029016\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.011074\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.006073\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.129000\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.016975\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.058403\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.016227\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.032555\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.016845\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.009403\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.026266\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.019989\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.022707\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002686\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.071345\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.001325\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.005901\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.008577\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.033914\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.013468\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.002973\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.008086\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.038729\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.038547\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.001939\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.028501\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.002661\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.018880\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.036161\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.005520\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.001933\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.071901\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.051367\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.014113\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.000939\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.004352\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.007099\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.013937\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.039697\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.003026\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.017100\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.003567\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.041504\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.016136\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.004500\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.015723\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.011146\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.005116\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.035907\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.005170\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.059295\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.110174\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.012357\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.000695\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.011018\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.027914\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.001051\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.000977\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.005379\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.007951\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.017958\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.004327\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.019295\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.004456\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.000732\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.006487\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.003424\n",
      "\n",
      "Test set: Average loss: 0.0307, Accuracy: 9914/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.012439\n",
      "Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.001688\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.010226\n",
      "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.011446\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.052678\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.007835\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.075945\n",
      "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.008909\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.097391\n",
      "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.001342\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.005126\n",
      "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.082278\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.042794\n",
      "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.014280\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.002641\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.033888\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.001554\n",
      "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.023429\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.001572\n",
      "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.004964\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.005120\n",
      "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.019602\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.063534\n",
      "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.019861\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.065030\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.018223\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.009468\n",
      "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.050648\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.007113\n",
      "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.170152\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.003622\n",
      "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.006488\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.071727\n",
      "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.008196\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.007988\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.017071\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.000942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.005141\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.003779\n",
      "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.002934\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.006800\n",
      "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.047343\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.001191\n",
      "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.019012\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.006645\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.000137\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.006967\n",
      "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.003553\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.021704\n",
      "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.022560\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.030018\n",
      "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.001215\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.031195\n",
      "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.001450\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.008480\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.016842\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.000452\n",
      "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.004038\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.105583\n",
      "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.016528\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.006719\n",
      "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.000270\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.000290\n",
      "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.003075\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.032177\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.000368\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.002065\n",
      "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.065894\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.009632\n",
      "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.025741\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.001057\n",
      "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.003872\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.005388\n",
      "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.031410\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.009584\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.001005\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.044367\n",
      "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.014027\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.003890\n",
      "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.005637\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.011318\n",
      "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.013704\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.005449\n",
      "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.114043\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.001615\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.027367\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.002239\n",
      "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.013494\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.006572\n",
      "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.019175\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.000424\n",
      "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.059437\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.021753\n",
      "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.035443\n",
      "\n",
      "Test set: Average loss: 0.0298, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.036289\n",
      "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.035782\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.043232\n",
      "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.041463\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.003876\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.076924\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.014488\n",
      "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.069721\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.004677\n",
      "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.020781\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.002191\n",
      "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.013714\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.007592\n",
      "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.005913\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.006738\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.017906\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.004163\n",
      "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.002633\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.001372\n",
      "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.004657\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.027767\n",
      "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.052924\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.001638\n",
      "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.021844\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.025126\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.001527\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.020073\n",
      "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.032812\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.078298\n",
      "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.013207\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.009398\n",
      "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.005469\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.001447\n",
      "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.001140\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.237396\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.000908\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.007897\n",
      "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.004322\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.024902\n",
      "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.008173\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.005345\n",
      "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.091175\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.003891\n",
      "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.011780\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.027326\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.019221\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.000543\n",
      "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.022976\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.009986\n",
      "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.010681\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.003869\n",
      "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.017675\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.000753\n",
      "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.006591\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.003384\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.000444\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.019696\n",
      "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.000420\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.005208\n",
      "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.016719\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.000082\n",
      "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.008786\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.006488\n",
      "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.061496\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.010165\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.008256\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.002821\n",
      "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.000166\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.002918\n",
      "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.022683\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.004822\n",
      "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.005355\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.037294\n",
      "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.013143\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.006544\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.004560\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.006096\n",
      "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.002207\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.000403\n",
      "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.005135\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.062918\n",
      "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.010750\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.001062\n",
      "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.007839\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.003332\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.003374\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.010743\n",
      "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.000849\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.087719\n",
      "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.034147\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.023383\n",
      "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.008042\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.084127\n",
      "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.058019\n",
      "\n",
      "Test set: Average loss: 0.0300, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.018697\n",
      "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.088290\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.001436\n",
      "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.000765\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.036961\n",
      "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.027954\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.056189\n",
      "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.010600\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.027311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.001477\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.074944\n",
      "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.039781\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.002461\n",
      "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.013462\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.004000\n",
      "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.016036\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.005625\n",
      "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.007050\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.007685\n",
      "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.003282\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.006237\n",
      "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.032054\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.015381\n",
      "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.030208\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.011388\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.006131\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.001500\n",
      "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.008928\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.011082\n",
      "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.001010\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.011329\n",
      "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.001049\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.035557\n",
      "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.002142\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.008452\n",
      "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.229061\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.002213\n",
      "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.000209\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.064009\n",
      "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.000866\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.001905\n",
      "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.141193\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.004207\n",
      "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.048570\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.002439\n",
      "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.000368\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.076584\n",
      "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.104242\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.010132\n",
      "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.052015\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.031703\n",
      "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.001073\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.022433\n",
      "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.000698\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.005780\n",
      "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.015153\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.008973\n",
      "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.003535\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.000474\n",
      "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.109413\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.027653\n",
      "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.001728\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.027807\n",
      "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.040991\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.004770\n",
      "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.001345\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.076584\n",
      "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.001189\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.031421\n",
      "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.009675\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.006897\n",
      "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.071306\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.001823\n",
      "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.000817\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.036045\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.006568\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.006756\n",
      "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.001050\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.009843\n",
      "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.003125\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.070552\n",
      "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.133693\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.008086\n",
      "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.061017\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.045010\n",
      "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.077671\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.027947\n",
      "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.002086\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.002733\n",
      "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.000549\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.059413\n",
      "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.025502\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.000438\n",
      "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.003960\n",
      "\n",
      "Test set: Average loss: 0.0300, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.010592\n",
      "Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.000341\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.004933\n",
      "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.007902\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.001404\n",
      "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.027335\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.004274\n",
      "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.006269\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.111229\n",
      "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.020996\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.000952\n",
      "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.057150\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.004131\n",
      "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.018549\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.009829\n",
      "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.001384\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.003606\n",
      "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.094160\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.037511\n",
      "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.011660\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.003445\n",
      "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.043354\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.002977\n",
      "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.001474\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.025331\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.004875\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.012383\n",
      "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.021657\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.000463\n",
      "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.020912\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.001418\n",
      "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.010938\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.000829\n",
      "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.018291\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.015604\n",
      "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.003358\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.093223\n",
      "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.031984\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.020341\n",
      "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.007781\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.002039\n",
      "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.000956\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.030900\n",
      "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.013623\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.008301\n",
      "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.000428\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.051999\n",
      "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.001235\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.006623\n",
      "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.001585\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.005893\n",
      "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.002613\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.017783\n",
      "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.001854\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.005943\n",
      "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.000761\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.009040\n",
      "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.007803\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.033431\n",
      "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.011292\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.004594\n",
      "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.037325\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.000688\n",
      "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.003220\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.005101\n",
      "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.013757\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.006683\n",
      "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.004326\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.016743\n",
      "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.002037\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.000605\n",
      "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.002048\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.011862\n",
      "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.011587\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.002827\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.002734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.004727\n",
      "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.006843\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.064283\n",
      "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.016494\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.003054\n",
      "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.006907\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.002740\n",
      "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.022670\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.014527\n",
      "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.006328\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.003450\n",
      "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.010034\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.012088\n",
      "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.004099\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.005905\n",
      "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.001811\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.008665\n",
      "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.031875\n",
      "\n",
      "Test set: Average loss: 0.0297, Accuracy: 9913/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# Notebook specific hack\n",
    "import sys; sys.argv=['']; del sys\n",
    "mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate an random image for which the model is highly confident yet it's completely absurd. This new image is out of distribution yet the model does not know it. We want to avoid doing such mistakes in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Model says : This is a 3 with probability 99.99%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=240x240 at 0x7FF770DFFDD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collapse\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn.parameter import Parameter\n",
    "from torchvision import transforms\n",
    "\n",
    "mnist_model = Net()\n",
    "mnist_model.load_state_dict(torch.load('mnist_cnn.pt'))\n",
    "\n",
    "dummy_input = Parameter(torch.rand(1, 1, 28, 28, requires_grad=True))\n",
    "\n",
    "lr = 1\n",
    "optimizer = optim.Adadelta([dummy_input], lr=lr)\n",
    "for i in range(100):\n",
    "    output = mnist_model(dummy_input)\n",
    "    entropy = Categorical(logits = output).entropy()\n",
    "    # print(f'Entropy {entropy.item():.2f}')\n",
    "    optimizer.zero_grad()\n",
    "    entropy.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "MAX = output[0].exp().max(dim=-1)\n",
    "print(f\"MNIST Model says : This is a {MAX.indices.item()} with probability {MAX.values.item() * 100:.2f}%\")\n",
    "pil_img = transforms.Resize((240, 240))(transforms.ToPILImage()(dummy_input[0]))\n",
    "display(pil_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other out of distribution detector have been proposed. Here is a sample of methods:\n",
    "\n",
    "- [Likelihood Ratios for Out-of-Distribution Detection](https://arxiv.org/pdf/1906.02845.pdf): Propose to learn 2 distinct models, one \"raw\", one with perturbation instilled into the dataset, and look at the log likelihood ratio of the two models, claim is that the difference between the two will reflect how \"far\" input is from the semantic part of the manifold of X. $p(x) = p(x_background)p(x_semantic)$, the perturbation needs to lie only on $x_semantic$.\n",
    "- [Out-of-distribution Detection in Classifiers via Generation](https://arxiv.org/pdf/1910.04241.pdf): Propose to use autoencoder (or GANs) to generate a low dimensional representation of the manifold of the dataset X, then perturb X on that representation. Those perturbated examples are trained to become a new \"class\" of the output of the classifier. \n",
    "- [Enhancing the reliability of Out-of-Distribution Image Detection in Neural Networks (Odin)](https://arxiv.org/pdf/1706.02690.pdf): This one uses temperature scaling regarding softmax to generate perturbated input, then look at the probability of the softmax if it passes a threshold. IMO, this paper is interesting as it supposes smoothness properties on In distribution data, and less smooth for out-of-distribution. It does require some examples of out-of-distribution for fitting 3 hyperparameters (temperature, threshold and magnitude of perturbation)\n",
    "- [Your classifier is secretly an energy based model and you should treat it like one](https://openreview.net/pdf?id=Hkxzx0NtDB): This one adds a new term in the loss to estimate p(x) basically. Multiple ood detectors are proposed, the most efficient being the second derivative of p(x), claiming again that density of p(x) will change more widly in ood space, leading to a good ood detector.\n",
    "\n",
    "- [WAIC, but Why? Generative Ensembles for Robust Anomaly Detection](https://arxiv.org/pdf/1810.01392.pdf): This paper proposes to use an ensemble of models and look at WAIC criterion to detect OOD. It makes many comparison to VAE and GANs\n",
    "\n",
    "- [Learning Confidence for Out-of-Distribution Detection in Neural Networks](https://arxiv.org/pdf/1802.04865v1.pdf) : The core idea in this paper is to change the learning loss, to learn confidence as prior task to classification task, a model is allowed to see real label only when it claims it can solve the problem, outputting via another head directly a confidence score. Caveat is that the model might choose to give up and always claim confidence, and another trick is proposed to emphasize the in-distribution vs out-of-distribution by preprocessing inputs to move them towards region of higher confidence. In-distribution tends to move closer to 1 than out-of-distribution. So the direct confidence estimator seems to be *smoother* out-of-distribution than in-distribution, where peaks are more likely to be found.\n",
    "\n",
    "- [Papers with code](https://paperswithcode.com/task/out-of-distribution-detection): More links on that hopefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tl;dr : Make two similar models, with two different random initialization, then train them at the same time.\n",
    "> The ood detector will simply be the a threshold classifier on the KL-divergence between the two outputs.**\n",
    "\n",
    "The core argument for this approach is that the neural network captures the dataset manifold (which means it will produce \"regular\" outputs for in dataset items). For the range of possible values it has random values for a random initialization. If that is true, then we train the model, we shift it's output only on the dataset manifold, and not anywhere else. If that assumption is correct, then the 2 models have very low probability of concurring in their output outside of the manifold if they have been initialized differently.\n",
    "\n",
    "It's quite close to WAIC, *but* the two models need to be trained at the same time. The argument is that is should align gradients during the training phase, leading to more correlation for in-dataset prediction for the models. The argument for this supposes that the lottery ticket hypothesis is true, and adds that lottery ticket is unique (or at least that the class of lottery tickets is very thin, and they all highly correlate to each other). If this is true, then the gradients within the network that correspond to this lottery ticket winner in *both* networks should be the same (or highly correlated).\n",
    "\n",
    "In order to fix the threshold, we found that simply setting it to be 10x the average kl-divergence obtained on the train dataset worked pretty well. As kl divergence is measured in bits, 10x is a quite large margin. More work could be done to study more closely the behaviour of this self kl-divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "\n",
    "MNIST attack like failure presented before.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNet(nn.Module):\n",
    "    def __init__(self, *models):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return [model(x) for model in self.models]\n",
    "\n",
    "    \n",
    "def train_multi(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = sum(F.nll_loss(output, target) for output in outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test_multi(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            test_loss += sum(F.nll_loss(output, target, reduction='sum').item() for output in outputs)\n",
    "            pred = outputs[0].argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def mnist_multi():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "\n",
    "    parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(os.path.expanduser('../data'), train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    model1 = Net()\n",
    "    model2 = Net()\n",
    "    model = MultiNet(model1, model2).to(device)\n",
    "\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_multi(args, model, device, train_loader, optimizer, epoch)\n",
    "        test_multi(args, model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_multi_cnn.pt\")\n",
    "        \n",
    "\n",
    "# mnist_multi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Notebook specific hack\n",
    "import sys; sys.argv=['']; del sys\n",
    "mnist_multi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0069, len 10000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "def kl(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            \n",
    "            test_loss += 1/2 * (F.kl_div(outputs[0], outputs[1].exp(), reduction='sum').item() + F.kl_div(outputs[1], outputs[0].exp(), reduction='sum').item())\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, len {} \\n'.format(\n",
    "        test_loss, len(test_loader.dataset)))\n",
    "    return test_loss\n",
    "    \n",
    "multi_model = MultiNet(Net(), Net())\n",
    "multi_model.load_state_dict(torch.load('mnist_multi_cnn.pt'))\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(os.path.expanduser('../data'), train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=1000, shuffle=True)\n",
    "ref_kl_loss = kl(multi_model, device='cpu', test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 2 models capable of detecting digits, we have instantly 3 checks for checking if the output of our model is valid. The 2 models need to be concording (they need to outputs the same digit as an output), they need to have similar kl-divergence, we actually have a reference for the test set, so we know what kind of divergence we should look for, anything 10x more is definitely ood (we could look at the test set distribution for more fine grain estimation). Because kl divergence is asymetric we have 2 values (it's harder for spiked distribution to have another distribution be close in the kl sense, so taking the max of kl-divergence should be used for out-of-distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn.parameter import Parameter\n",
    "from torchvision import transforms\n",
    "\n",
    "def attack(loss_fn, verbose=True, n=100, lr=1):\n",
    "    multi_model = MultiNet(Net(), Net())\n",
    "    multi_model.load_state_dict(torch.load('mnist_multi_cnn.pt'))\n",
    "\n",
    "    dummy_input = Parameter(torch.rand(1, 1, 28, 28, requires_grad=True))\n",
    "\n",
    "    optimizer = optim.Adadelta([dummy_input], lr=lr)\n",
    "    for i in range(n):\n",
    "        outputs = multi_model(dummy_input)\n",
    "        \n",
    "        loss = loss_fn(outputs)\n",
    "        # print(f'Entropy {entropy.item():.2f}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    MAX1 = outputs[0][0].exp().max(dim=-1)\n",
    "    MAX2 = outputs[1][0].exp().max(dim=-1)\n",
    "\n",
    "    kl_loss = F.kl_div(outputs[0], outputs[1].exp(), reduction='batchmean')\n",
    "    kl_loss2 = F.kl_div(outputs[1], outputs[0].exp(), reduction='batchmean')\n",
    "\n",
    "    if (kl_loss / ref_kl_loss) > 10 or kl_loss2 / ref_kl_loss > 10 or MAX1.indices.item() != MAX2.indices.item():\n",
    "        success = False\n",
    "    else:\n",
    "        success = True\n",
    "    if verbose:\n",
    "        print(f\"MNIST Model says : This is a {MAX1.indices.item()} with probability {MAX1.values.item() * 100:.2f}%\")\n",
    "        print(f\"MNIST Model 2 says : This is a {MAX2.indices.item()} with probability {MAX2.values.item() * 100:.2f}%\")\n",
    "\n",
    "        print(f\"KL-divergence is {kl_loss / ref_kl_loss} {kl_loss2 / ref_kl_loss}\")\n",
    "\n",
    "        if success:\n",
    "            print(\"ATTACK SUCCEEDED\")\n",
    "        else:\n",
    "            print(\"ATTACK FAILED\")\n",
    "        pil_img = transforms.Resize((240, 240))(transforms.ToPILImage()(dummy_input[0]))\n",
    "        display(pil_img)\n",
    "    return success\n",
    "        \n",
    "        \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we simply attack the first model like we did earlier, we can see that we can trick it as easily as before. *BUT* the second model, does not get attacked which is to be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Model says : This is a 5 with probability 99.32%\n",
      "MNIST Model 2 says : This is a 1 with probability 51.35%\n",
      "KL-divergence is 879.1231689453125 221.9772186279297\n",
      "ATTACK FAILED\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=240x240 at 0x7F995EAD69D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(outputs):\n",
    "    entropy = Categorical(logits = outputs[0]).entropy()\n",
    "    \n",
    "    loss = entropy\n",
    "    return loss\n",
    "\n",
    "attack(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we try a smarter and attack __both__ models at the same time, we can't succeed at a consistent rate. Be warned, it will succeed sometimes, just not consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Model says : This is a 7 with probability 11.34%\n",
      "MNIST Model 2 says : This is a 1 with probability 11.91%\n",
      "KL-divergence is 0.8557947278022766 0.8770337104797363\n",
      "ATTACK FAILED\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=240x240 at 0x7F995EACC650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(outputs):\n",
    "    entropy1 = Categorical(logits = outputs[0]).entropy()\n",
    "    entropy2 = Categorical(logits = outputs[1]).entropy()\n",
    "\n",
    "    kl_loss1 = F.kl_div(outputs[0], outputs[1].exp(), reduction='batchmean')\n",
    "    kl_loss2 = F.kl_div(outputs[1], outputs[0].exp(), reduction='batchmean')\n",
    "\n",
    "    distance = F.mse_loss(outputs[0], outputs[1])\n",
    "\n",
    "    loss = entropy1 + entropy2  + kl_loss1 + kl_loss2 + distance\n",
    "    return loss\n",
    "    \n",
    "_ = attack(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be warned, it will succeed sometimes, just not consistently. For comparison, the first attack succeeds with close to 100% (we couldn't make it fail). Actually because we have 10 classes, and if we supposed out-of-distribution probability distribution is uniformly random, it should be something close to 10%, when our initial random image finds a place where the 2 models intersect on the same digit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........FF.F...F.FF.F..F....FF...F..FF..F.F.F.FF....F....F.......FF......FF.F.FFF.FF..F............Attack success rate 31.00%\n"
     ]
    }
   ],
   "source": [
    "def loss(outputs):\n",
    "    entropy1 = Categorical(logits = outputs[0]).entropy()\n",
    "    entropy2 = Categorical(logits = outputs[1]).entropy()\n",
    "\n",
    "    kl_loss1 = F.kl_div(outputs[0], outputs[1].exp(), reduction='batchmean')\n",
    "    kl_loss2 = F.kl_div(outputs[1], outputs[0].exp(), reduction='batchmean')\n",
    "\n",
    "    distance = F.mse_loss(outputs[0], outputs[1])\n",
    "\n",
    "    loss = entropy1 + entropy2  + kl_loss1 + kl_loss2 + distance\n",
    "    return loss\n",
    "    \n",
    "attacks = []\n",
    "for i in range(100):\n",
    "    success = attack(loss, verbose=False, n=200, lr=0.1)\n",
    "    if success:\n",
    "        print(\"F\", end='')\n",
    "    else:\n",
    "        print(\".\", end='')\n",
    "    attacks.append(success)\n",
    "\n",
    "print('')\n",
    "print(f\"Attack success rate {sum(attacks)/len(attacks) * 100:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual attack range seems to stagnate at around 30% (max observed 41%) with various learning rates and attack steps. There probably are better strategies to attack, this, but the main point is that it became __harder__. This is expected to be a property of the *output landscape*, as the number of classes increases, it should become harder still. Yet as the 30% is larger than the expected 10%, something else might be at play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2\n",
    "\n",
    "Now let's test this on common ood detection for classic datasets. We will add ood detection for the train dataset, just to check that we don't *exclude* too much of the original dataset. Datasets used will be MNIST, FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "from torchvision.datasets import MNIST, Omniglot, FashionMNIST\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "datasets = [MNIST, FashionMNIST]\n",
    "\n",
    "def dataset_multi(dataset_cls, filename):\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1e-2, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "\n",
    "    parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset_cls('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "                dataset_cls('../data', train=False, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                    ])), batch_size = args.test_batch_size)\n",
    "\n",
    "    model1 = Net()\n",
    "    model2 = Net()\n",
    "    model = MultiNet(model1, model2).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = optim.lr_scheduler.CyclicLR(                             \n",
    "          optimizer, base_lr=0, max_lr=args.lr, cycle_momentum=False, step_size_up=200                    )\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_multi(args, model, device, train_loader, optimizer, epoch)\n",
    "        test_multi(args, model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def run_datasets():\n",
    "    for dataset_cls in datasets:\n",
    "        filename = f'{dataset_cls.__name__}.pt'\n",
    "\n",
    "        if os.path.exists(filename):\n",
    "            continue\n",
    "\n",
    "        dataset_multi(dataset_cls, filename)\n",
    "\n",
    "# run_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 4.678443\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 4.658801\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 4.649020\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 4.619276\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 4.667685\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 4.653006\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 4.629105\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 4.640399\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 4.653443\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 4.598194\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 4.622351\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 4.648350\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 4.637665\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 4.657411\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 4.681814\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 4.612526\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 4.648591\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 4.620812\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 4.620546\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 4.646947\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 4.655905\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 4.601874\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 4.581450\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 4.643826\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 4.648104\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 4.630174\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 4.654440\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 4.630270\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 4.642820\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 4.631962\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 4.672729\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 4.655988\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 4.602872\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 4.651089\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 4.653541\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 4.638262\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 4.621167\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 4.619888\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 4.636371\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 4.636390\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 4.621562\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 4.640183\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 4.619219\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 4.638980\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 4.616252\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 4.618689\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 4.650091\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 4.607698\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 4.680491\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 4.665526\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 4.643156\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 4.635918\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 4.702347\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 4.623566\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 4.644181\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 4.578151\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 4.651415\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 4.609938\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 4.640367\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 4.622552\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 4.642035\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 4.619561\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 4.648114\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 4.623934\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 4.626898\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 4.545144\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 4.641191\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 4.619653\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 4.612524\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 4.648462\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 4.630075\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 4.667891\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 4.632173\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 4.615168\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 4.615157\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 4.657790\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 4.629138\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 4.601340\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 4.636714\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 4.642306\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 4.595216\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 4.588351\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 4.641157\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 4.572662\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 4.635239\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 4.693813\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 4.635779\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 4.633232\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 4.600832\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 4.620749\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 4.624218\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 4.622747\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 4.626929\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 4.632570\n",
      "\n",
      "Test set: Average loss: 4.6212, Accuracy: 1017/10000 (10%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 4.625928\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 4.370370\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 3.974853\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 3.670844\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 3.171309\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 2.536248\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 2.250967\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 1.885509\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.330160\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 1.797965\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.083718\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.932460\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 1.058964\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 1.223677\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 1.192665\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 1.350343\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.960846\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.776129\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 1.255197\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.972844\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.073612\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.959402\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 1.150655\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.780110\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 1.057461\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.711419\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.776742\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.604462\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.742161\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 1.174043\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.559632\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.781620\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.726540\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.557718\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.657212\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.977887\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.582461\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.817567\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.601662\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.683241\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.612550\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.374243\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.604217\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.704221\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.612204\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.931486\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.668947\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.809987\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.533666\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.578252\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.897098\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.715055\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 1.005558\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.615000\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.642142\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.661747\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.635782\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.645785\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.783740\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.694865\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.536647\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.704715\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.831203\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.904083\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.669192\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.545743\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.679076\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.447522\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.597307\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.527995\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.721736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.477223\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.331496\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.566394\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.843693\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.751386\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.616609\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.432645\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.572365\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.685344\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.593223\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.284163\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.675099\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.393342\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.268499\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.454146\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.378899\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.781233\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.507888\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.607300\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.607787\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.412762\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.540949\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.775353\n",
      "\n",
      "Test set: Average loss: 0.3297, Accuracy: 9511/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.212384\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.657364\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.720067\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.488986\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.307274\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.783704\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.665509\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.552071\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.459422\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.226836\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.628532\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.452639\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.345213\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.451160\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.616178\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.339868\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.730246\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.566034\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.456342\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.319424\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.302344\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.304703\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.598573\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.228534\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.658867\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.433473\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.453821\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.299301\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.530184\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.427014\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.436949\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.438716\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.329303\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.515603\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.345174\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.286301\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.283660\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.522492\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.531442\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.387465\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.360263\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.442758\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.293689\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.492392\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.244527\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.135002\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.442797\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.251371\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.394073\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.385365\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.428025\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.437317\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.245778\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.151269\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.162877\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.253813\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.125163\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.264391\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.311544\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.435709\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.450469\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.378751\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.263603\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.220422\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.174424\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.201340\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.152769\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.203444\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.157698\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.235217\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.375683\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.131579\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.276328\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.521067\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.489321\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.199474\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.196320\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.414515\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.110518\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.290449\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.305800\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.175782\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.091989\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.251184\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.183172\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.265859\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.261185\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.145336\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.101195\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.466688\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.254777\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.150596\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.427436\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.154955\n",
      "\n",
      "Test set: Average loss: 0.1541, Accuracy: 9742/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.209858\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.165632\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.150082\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.191739\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.181602\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.363191\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.083858\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.133952\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.227604\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.193689\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.308908\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.321573\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.285039\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.268442\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.463324\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.319850\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.357287\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.228435\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.233792\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.432497\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.287049\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.194750\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.272798\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.202399\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.289054\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.383221\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.451608\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.227002\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.254288\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.081951\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.287121\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.243270\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.237291\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.190258\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.417398\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.291884\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.064824\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.146849\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.310337\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.202404\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.237402\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.795139\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.358911\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.185341\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.391812\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.805690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.502126\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.102437\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.131273\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.320770\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.209073\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.067102\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.172401\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.188451\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.152724\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.224100\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.125329\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.190523\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.184507\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.109688\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.164217\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.096106\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.103079\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.181615\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.102945\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.126091\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.629287\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.278333\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.228853\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.321715\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.205919\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.269166\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.074612\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.160712\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.162510\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.313620\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.481478\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.083307\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.199219\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.132514\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.098169\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.067911\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.345524\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.218872\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.159744\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.331436\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.336216\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.140354\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.405524\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.277542\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.109413\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.155273\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.252962\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.209610\n",
      "\n",
      "Test set: Average loss: 0.1020, Accuracy: 9822/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.111037\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.133428\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.178597\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.106679\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.131958\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.201708\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.194182\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.110849\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.096191\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.391774\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.084390\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.188230\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.172440\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.099224\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.094384\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.094614\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.271509\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.135229\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.398700\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.099791\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.185039\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.128134\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.095132\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.303161\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.084906\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.331384\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.301745\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.266353\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.141757\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.096218\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.137503\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.059474\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.418248\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.122358\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.152683\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.021923\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.304519\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.098203\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.207343\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.088753\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.161585\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.129001\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.069161\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.084419\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.194483\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.127542\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.328382\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.186839\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.176389\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.198191\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.195122\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.315572\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.122571\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.255825\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.115287\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.119032\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.048048\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.127158\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.206838\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.183599\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.119479\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.174666\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.093779\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.058813\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.107260\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.162518\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.340375\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.160529\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.114009\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.202314\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.070784\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.198929\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.156864\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.279587\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.148615\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.049731\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.123398\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.166279\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.186163\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.060604\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.104908\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.146108\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.138700\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.227299\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.188365\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.107098\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.045661\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.450471\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.230941\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.106704\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.172444\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.185178\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.126067\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.164544\n",
      "\n",
      "Test set: Average loss: 0.0890, Accuracy: 9850/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.212216\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.074707\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.146027\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.055550\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.153055\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.174204\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.076034\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.033420\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.081288\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.196459\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.261815\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.203047\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.194704\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.185134\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.083198\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.356736\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.055359\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.292941\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.151015\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.031959\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.231490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.040661\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.183396\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.157757\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.124423\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.203818\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.256351\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.396387\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.236342\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.249214\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.064175\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.253521\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.057811\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.143910\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.109388\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.093677\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.217357\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.146043\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.195511\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.109836\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.137149\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.099823\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.256457\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.392414\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.076069\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.261835\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.050154\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.319995\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.103539\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.152302\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.241866\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.072251\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.320721\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.065337\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.219380\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.335639\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.196223\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.157373\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.303421\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.231654\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.097981\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.087559\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.255957\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.076867\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.125808\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.364370\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.103001\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.050209\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.124404\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.260181\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.341820\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.033475\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.122042\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.053728\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.037872\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.176089\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.191331\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.220902\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.431255\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.068946\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.145256\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.187146\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.211797\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.023542\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.093908\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.228114\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.076152\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.212416\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.143650\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.095405\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.348235\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.033774\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.165161\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.390175\n",
      "\n",
      "Test set: Average loss: 0.0766, Accuracy: 9876/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.068766\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.169754\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.082411\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.033218\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.077610\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.157421\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.067151\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.072438\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.077376\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.042090\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.345564\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.136219\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.099471\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.092632\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.112510\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.099389\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.152493\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.558217\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.078000\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.341250\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.058495\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.136524\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.033738\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.056174\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.068844\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.198354\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.030190\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.145152\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.095984\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.156103\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.606255\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.093291\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.058110\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.082033\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.137942\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.056786\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.063657\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.054909\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.070699\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.089461\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.112668\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.112494\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.081702\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.255041\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.239346\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.185983\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.055435\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.055194\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.140312\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.132568\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.088397\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.136190\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.105207\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.040592\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.161080\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.030436\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.040507\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.056304\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.028082\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.059875\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.477687\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.164387\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.077375\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.211828\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.097289\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.080655\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.087010\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.060213\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.284032\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.130050\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.090746\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.085229\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.173569\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.146094\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.061910\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.213885\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.081509\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.069553\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.119007\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.196962\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.145507\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.091293\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.088021\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.165579\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.116814\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.386247\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.083434\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.085174\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.069519\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.038579\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.028825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.040046\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.050925\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.088096\n",
      "\n",
      "Test set: Average loss: 0.0751, Accuracy: 9866/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.101928\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.077202\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.081408\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.089856\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.021517\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.465495\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.042327\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.108641\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.105622\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.035720\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.130676\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.147733\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.139844\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.071648\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.023821\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.252280\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.081907\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.205269\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.023331\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.026428\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.070055\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.164695\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.054831\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.014896\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.120324\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.136741\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.108893\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.164173\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.185454\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.087581\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.316479\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.128897\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.295346\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.082114\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.352438\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.246302\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.154249\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.072073\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.127535\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.237811\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.082740\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.112318\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.376594\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.062063\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.081373\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.063296\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.106396\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.036526\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.015251\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.288005\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.696770\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.141046\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.152187\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.089681\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.033750\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.181647\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.023298\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.218540\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.270545\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.100852\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.137491\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.101815\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.151375\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.343078\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.278898\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.095612\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.085326\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.041925\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.038106\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.194002\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.122903\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.021745\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.158602\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.340176\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.077186\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.062800\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.069266\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.066641\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.163584\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.250189\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.079847\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.082208\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.291331\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.154660\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.111925\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.109763\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.110370\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.153663\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.239730\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.079732\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.366609\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.144851\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.116881\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.086247\n",
      "\n",
      "Test set: Average loss: 0.0695, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.184780\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.025163\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.272623\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.205018\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.203095\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.057972\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.167537\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.162251\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.038774\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.257141\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.378357\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.064039\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.067649\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.054996\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.037227\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.066075\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.048801\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.029466\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.046827\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.092087\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.151335\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.151894\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.206037\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.044279\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.083955\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.115733\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.106796\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.068786\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.090843\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.321645\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.101840\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.257356\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.099732\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.074589\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.238013\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.095688\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.156024\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.242865\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.120274\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.164413\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.077476\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.110297\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.171632\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.083421\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.128101\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.195530\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.027102\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.036026\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.019765\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.215432\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.150720\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.291641\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.091147\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.125595\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.024948\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.160900\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.093412\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.213666\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.112096\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.273763\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.074300\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.219230\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.084043\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.045221\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.127915\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.287659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.350248\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.086167\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.249401\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.329739\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.324897\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.023771\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.047472\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.048374\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.335635\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.022069\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.216971\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.024891\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.073600\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.050023\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.085659\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.020467\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.042854\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.142207\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.419847\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.159620\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.110622\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.066862\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.119268\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.165137\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.083964\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.208919\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.139676\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.042949\n",
      "\n",
      "Test set: Average loss: 0.0760, Accuracy: 9862/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.204684\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.102169\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.037033\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.042115\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.162223\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.057641\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.023981\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.149160\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.067993\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.018789\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.112582\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.081057\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.091618\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.091764\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.071889\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.191104\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.255439\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.032869\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.149891\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.054877\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.070501\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.110481\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.044557\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.095451\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.018255\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.050455\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.061951\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.062666\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.130030\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.085973\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.030205\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.064551\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.401693\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.431334\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.150820\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.124487\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.227289\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.089696\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.179589\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.152535\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.222531\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.176169\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.010409\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.248184\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.145389\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.016345\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.023518\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.254452\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.092805\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.230213\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.051452\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.059405\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.024193\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.048957\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.417306\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.035465\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.071371\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.140618\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.033007\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.162971\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.440193\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.094507\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.201991\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.094184\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.092289\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.091448\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.037919\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.323793\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.433393\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.160441\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.078623\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.040138\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.094643\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.065090\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.113994\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.101154\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.247407\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.138535\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.040388\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.098810\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.079942\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.078085\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.074832\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.169754\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.058565\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.032048\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.047729\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.244736\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.281474\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.138213\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.259932\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.014642\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.167248\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.333321\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 9885/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.038167\n",
      "Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.112630\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.056463\n",
      "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.114008\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.044523\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.036970\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.087059\n",
      "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.890041\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.167611\n",
      "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.156332\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.093640\n",
      "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.099335\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.105854\n",
      "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.235547\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.113444\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.089973\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.245616\n",
      "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.040597\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.163924\n",
      "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.026771\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.052321\n",
      "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.081132\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.067478\n",
      "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.172168\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.079329\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.056759\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.048370\n",
      "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.014097\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.075538\n",
      "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.385539\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.182291\n",
      "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.195708\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.117589\n",
      "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.241472\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.254201\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.161440\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.097832\n",
      "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.160469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.076742\n",
      "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.049145\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.082274\n",
      "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.057610\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.400132\n",
      "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.028930\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.028511\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.051149\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.008368\n",
      "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.082019\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.014026\n",
      "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.010097\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.059406\n",
      "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.117745\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.054272\n",
      "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.145846\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.052626\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.059534\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.104537\n",
      "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.198372\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.071457\n",
      "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.118196\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.068970\n",
      "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.101228\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.017186\n",
      "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.136152\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.329566\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.204913\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.009474\n",
      "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.448654\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.089206\n",
      "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.172514\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.027122\n",
      "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.185420\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.171439\n",
      "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.059845\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.408608\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.016981\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.148949\n",
      "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.140895\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.181275\n",
      "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.096145\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.046841\n",
      "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.070773\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.009048\n",
      "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.158135\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.121702\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.355580\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.143753\n",
      "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.039239\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.110437\n",
      "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.288536\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.296077\n",
      "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.070567\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.026877\n",
      "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.203594\n",
      "\n",
      "Test set: Average loss: 0.0658, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.059628\n",
      "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.105383\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.114581\n",
      "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.083951\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.051126\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.091611\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.098578\n",
      "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.042568\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.077711\n",
      "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.012690\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.010475\n",
      "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.111982\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.057811\n",
      "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.095258\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.061424\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.051241\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.027139\n",
      "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.032684\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.260455\n",
      "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.030184\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.211971\n",
      "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.058571\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.033416\n",
      "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.057555\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.119172\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.270179\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.275765\n",
      "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.029171\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.127002\n",
      "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.018270\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.271857\n",
      "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.040886\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.036979\n",
      "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.140797\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.193101\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.031586\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.116343\n",
      "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.277083\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.093691\n",
      "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.056854\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.028611\n",
      "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.088827\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.046235\n",
      "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.041121\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.108904\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.043407\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.089701\n",
      "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.096030\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.036343\n",
      "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.074367\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.115295\n",
      "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.068833\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.145501\n",
      "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.032402\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.131394\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.386752\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.186471\n",
      "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.065992\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.090950\n",
      "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.045342\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.049018\n",
      "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.130790\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.107189\n",
      "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.059357\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.143704\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.059437\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.045527\n",
      "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.121803\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.259971\n",
      "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.324591\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.146529\n",
      "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.067556\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.163029\n",
      "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.087139\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.158250\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.137210\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.094259\n",
      "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.096320\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.128912\n",
      "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.131796\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.007179\n",
      "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.233252\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.099207\n",
      "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.034517\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.164293\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.100710\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.038841\n",
      "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.015453\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.036079\n",
      "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.097192\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.110044\n",
      "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.070157\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.222209\n",
      "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.090465\n",
      "\n",
      "Test set: Average loss: 0.0841, Accuracy: 9886/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.095461\n",
      "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.057480\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.126880\n",
      "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.078294\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.047340\n",
      "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.040917\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.029982\n",
      "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.141370\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.071576\n",
      "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.075702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.006851\n",
      "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.302071\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.086137\n",
      "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.131942\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.034887\n",
      "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.228863\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.106835\n",
      "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.025735\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.040329\n",
      "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.096298\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.068281\n",
      "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.018559\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.064540\n",
      "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.030934\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.004238\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.216706\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.141115\n",
      "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.052886\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.094480\n",
      "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.085448\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.067714\n",
      "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.137622\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.147880\n",
      "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.182542\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.035963\n",
      "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.130752\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.060964\n",
      "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.208023\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.121492\n",
      "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.164051\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.055499\n",
      "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.162993\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.075843\n",
      "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.262387\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.090348\n",
      "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.049908\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.013955\n",
      "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.132728\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.016692\n",
      "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.112269\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.162871\n",
      "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.027268\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.013383\n",
      "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.021150\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.019456\n",
      "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.055161\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.027406\n",
      "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.116746\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.100720\n",
      "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.037382\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.039655\n",
      "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.345830\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.022501\n",
      "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.202101\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.272302\n",
      "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.039293\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.030263\n",
      "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.033300\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.024907\n",
      "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.046861\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.021316\n",
      "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.063342\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.192217\n",
      "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.101544\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.325007\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.354015\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.016279\n",
      "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.088345\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.212072\n",
      "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.218511\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.014657\n",
      "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.038324\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.084847\n",
      "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.021677\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.087572\n",
      "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.074200\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.199901\n",
      "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.083516\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.045267\n",
      "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.096323\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.442603\n",
      "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.012335\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.138841\n",
      "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.105540\n",
      "\n",
      "Test set: Average loss: 0.0737, Accuracy: 9864/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.018589\n",
      "Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.014235\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.094808\n",
      "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.060934\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.123986\n",
      "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.126088\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.112430\n",
      "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.123754\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.005789\n",
      "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.051952\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.005236\n",
      "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.198492\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.097614\n",
      "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.030656\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.073893\n",
      "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.023064\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.058159\n",
      "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.082832\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.176227\n",
      "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.157010\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.257015\n",
      "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.110094\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.157310\n",
      "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.043322\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.087574\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.053145\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.106386\n",
      "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.119743\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.057334\n",
      "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.039083\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.077440\n",
      "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.074942\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.019917\n",
      "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.192746\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.120216\n",
      "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.125422\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.041321\n",
      "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.040407\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.180454\n",
      "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.133180\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.076378\n",
      "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.196252\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.061418\n",
      "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.122496\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.105485\n",
      "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.132591\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.028138\n",
      "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.142822\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.168086\n",
      "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.036975\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.093937\n",
      "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.048505\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.256014\n",
      "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.231521\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.242912\n",
      "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.203768\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.051480\n",
      "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.054827\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.109224\n",
      "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.003766\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.016679\n",
      "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.147627\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.097323\n",
      "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.018670\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.066304\n",
      "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.067062\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.034244\n",
      "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.146456\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.022023\n",
      "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.298130\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.005075\n",
      "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.134913\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.332504\n",
      "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.079015\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.118052\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.047701\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.042880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.502742\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.103783\n",
      "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.329141\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.031755\n",
      "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.038848\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.125051\n",
      "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.089050\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.126881\n",
      "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.030019\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.058595\n",
      "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.069381\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.032277\n",
      "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.033214\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.059181\n",
      "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.227141\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.062593\n",
      "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.169951\n",
      "\n",
      "Test set: Average loss: 0.0643, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 4.673808\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 4.639294\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 4.611559\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 4.662404\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 4.607990\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 4.575208\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 4.599981\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 4.586050\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 4.707212\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 4.642479\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 4.637908\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 4.652577\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 4.604040\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 4.577564\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 4.683064\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 4.643386\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 4.683489\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 4.615004\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 4.687427\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 4.614288\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 4.615116\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 4.647669\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 4.629736\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 4.623862\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 4.611092\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 4.629498\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 4.640756\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 4.618609\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 4.610313\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 4.663370\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 4.615940\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 4.592754\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 4.636031\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 4.641484\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 4.639754\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 4.666304\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 4.598004\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 4.595850\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 4.675263\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 4.627089\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 4.651551\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 4.592568\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 4.623507\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 4.607434\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 4.680269\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 4.592320\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 4.679543\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 4.584314\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 4.671143\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 4.616817\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 4.667284\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 4.641726\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 4.624589\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 4.627250\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 4.649526\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 4.639769\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 4.629581\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 4.661208\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 4.653466\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 4.664478\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 4.588418\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 4.676417\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 4.646100\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 4.615290\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 4.607385\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 4.626709\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 4.630777\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 4.637348\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 4.655286\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 4.621845\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 4.686489\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 4.645214\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 4.634274\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 4.637311\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 4.654700\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 4.631264\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 4.624049\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 4.592087\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 4.646905\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 4.628182\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 4.611449\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 4.592940\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 4.599646\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 4.639396\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 4.644913\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 4.597026\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 4.586583\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 4.627360\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 4.697141\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 4.607797\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 4.623407\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 4.626312\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 4.647423\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 4.653293\n",
      "\n",
      "Test set: Average loss: 4.6201, Accuracy: 1007/10000 (10%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 4.687285\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 4.137927\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 3.599834\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 3.229969\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 2.459237\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 2.470401\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.935202\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 1.697838\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.721103\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 1.525273\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.526315\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 1.473468\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 1.595099\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 1.887759\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 1.567998\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 1.569514\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 1.585876\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 1.520529\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 1.087158\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 1.364609\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.842680\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 1.260930\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 1.595463\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 1.251796\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 1.297564\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 1.346273\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 1.118111\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 1.300894\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 1.226095\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 1.645816\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.312044\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 1.106326\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 1.299998\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 1.410080\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 1.000123\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 1.257186\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 1.719656\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 1.711781\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.995263\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.990201\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.410913\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 1.091731\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 1.400395\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 1.504956\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 1.274861\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.878062\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 1.023376\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 1.801244\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.836179\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.887839\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.350484\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 1.433613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 1.058681\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 1.064857\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.878353\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 1.490009\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 1.572579\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.926961\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.968857\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 1.077733\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.526482\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 1.607954\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 1.145802\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 1.163990\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.876294\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.866080\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 1.468978\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 1.351028\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 1.072245\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 1.068767\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.146088\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 1.270903\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 1.143090\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 1.242134\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.879064\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 1.067289\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.937620\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 1.014917\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 1.199255\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.656226\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.669759\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 1.110394\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 1.195370\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.960073\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.860258\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.861467\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.874613\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 1.061287\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 1.188096\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.840845\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.744925\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 1.079130\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.881594\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.885343\n",
      "\n",
      "Test set: Average loss: 0.8857, Accuracy: 8327/10000 (83%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.247481\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.989652\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.978845\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.867450\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.840526\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.918959\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.691177\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.732528\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 1.589595\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 1.266503\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.323517\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.937099\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.701927\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.899201\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 1.246670\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.951544\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.793149\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.765476\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 1.043690\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 1.210180\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.865885\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.761456\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.783317\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 1.334576\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.821394\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 1.158640\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 1.155150\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.981863\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 1.069142\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.719057\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.900956\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.572937\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 1.005208\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 1.442336\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.996083\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 1.295969\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.887763\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 1.075790\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.846021\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 1.158401\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.875793\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.867195\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 1.035531\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.934319\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.922292\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.765218\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 1.196449\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.697160\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.837029\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.778789\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.449189\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.617744\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.932662\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.615795\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.961578\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.825083\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.782782\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.893491\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.551749\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.686296\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.979159\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 1.234127\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.926795\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.845823\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.786444\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.710669\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.925907\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.736304\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 1.158061\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.766084\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.691115\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.943732\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.864911\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.872295\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.616640\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.920592\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.839680\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 1.026675\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 1.153960\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 1.345208\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.091152\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.990671\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.753871\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.770766\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.867740\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.874419\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.715652\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.908020\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.451735\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.380080\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.737290\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 1.113533\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.717572\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.888689\n",
      "\n",
      "Test set: Average loss: 0.7207, Accuracy: 8699/10000 (87%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.755884\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.814694\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.788624\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 1.421981\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 1.257706\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.681593\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.787245\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.981683\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.946980\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 1.109253\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.098882\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.629477\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.797183\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.825373\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 1.014673\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.686255\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.973558\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.973876\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.630632\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.589520\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.844949\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.606009\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.586190\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 1.134345\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.866703\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.597579\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.498270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.728239\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.956684\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.959414\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.770230\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.625525\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.903163\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.817079\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.725252\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.825675\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.467297\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.584116\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.741253\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.841790\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.037671\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.960517\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 1.186827\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.765556\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.558553\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.549768\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.670393\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.906705\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.379157\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.470401\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.873771\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.779885\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.823753\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 1.060528\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.623398\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 1.143236\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 1.033579\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.624137\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.685221\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.562764\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.730972\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.721879\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.433589\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.636844\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.532214\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.871971\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.701464\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.930681\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.760317\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.715588\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.806646\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.927849\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.733103\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.620003\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.754867\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.739450\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.551464\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 1.107842\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.781142\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.711723\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.595296\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.603108\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.697007\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.711765\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.604099\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.728966\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.405746\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.745448\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.830993\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.521457\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.681844\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.789873\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.695649\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.641235\n",
      "\n",
      "Test set: Average loss: 0.6412, Accuracy: 8821/10000 (88%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.507282\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.717787\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.926522\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.637778\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.714114\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.632864\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 1.010645\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.667785\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.577150\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.562733\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.931789\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.785088\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.691660\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.545836\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.591253\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.685803\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.620144\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.916168\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.442342\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.848541\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.902535\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.523064\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.719510\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.592388\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.920154\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.641323\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.726225\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.940485\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.667537\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.718354\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.677964\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.704342\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.567806\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.561176\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.763586\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.780028\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.536182\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 1.025029\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.891035\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.623782\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.502685\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.875721\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.452247\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.584696\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.767151\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.502551\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.769565\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.519043\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.645469\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.471975\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.715865\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.823273\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.610158\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.893911\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.589823\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.752697\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.567850\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.755755\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.868800\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.614990\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.922912\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.345835\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.743878\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.742238\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.717152\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.617360\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.545605\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.487891\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.447467\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.628796\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.454494\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.723864\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.666494\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.463419\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.531032\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.298209\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.733285\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.489181\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 1.009436\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.811044\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.708571\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.810082\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.461897\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.784208\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.734002\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.326604\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.577272\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.839516\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.516598\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.744540\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.653543\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 1.037845\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.782485\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.843068\n",
      "\n",
      "Test set: Average loss: 0.6066, Accuracy: 8892/10000 (89%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.685385\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.480228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.642837\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.601975\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.562313\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.558967\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.562836\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.226657\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.506545\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.464020\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.489343\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.848032\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.385828\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.364126\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.713487\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.778522\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.711092\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 1.024743\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.634985\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.628104\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.622564\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.662473\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.925962\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.509812\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.393297\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.285020\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.627523\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.944199\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.728752\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.578571\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.867691\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.796168\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.622732\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.470421\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.489313\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.491513\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.912078\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.590809\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.605312\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.514321\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.475464\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.405954\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.671383\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.489787\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 1.010236\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.518461\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.603244\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.212950\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.559809\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.575211\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.603035\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.374970\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.540601\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.618885\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.662237\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.487477\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.484022\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.324301\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.816869\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.728555\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.742795\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 1.056036\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.623103\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.823609\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.781290\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.494481\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.549184\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.475543\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.647706\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.640150\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.503083\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.420420\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.864668\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.998383\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.681059\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.797081\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.751496\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.413377\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.431238\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.294865\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.672264\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.527508\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.486220\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.711888\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.767294\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.759116\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.581039\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.619504\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.676292\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.462953\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.605765\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.839555\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.572656\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.684880\n",
      "\n",
      "Test set: Average loss: 0.5548, Accuracy: 8960/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.365711\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.509802\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.706070\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.973221\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.484886\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.581593\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.454730\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.603700\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.535227\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.336462\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.728544\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.419536\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.819183\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.479229\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.397057\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.548013\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.350086\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.485639\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.995257\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.638935\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.656619\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.426780\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.855860\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.576635\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.642867\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.711580\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.955721\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.408987\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.671721\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.617319\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.646578\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.594849\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.446191\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.678692\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.617829\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 1.060052\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.587142\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.451906\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.590276\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 1.098505\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.572825\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.497757\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.949840\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.757505\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.989198\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.409539\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.298849\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.682757\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.541697\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.353857\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.519510\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.437342\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.500952\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.750225\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.913685\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.475104\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.677848\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.366743\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.614930\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.537612\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.417075\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.543175\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.553607\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.447657\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.464725\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.648974\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.612126\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.511325\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.676854\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.519177\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.505660\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.542305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.719392\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.667354\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.464161\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.478197\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.271407\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.342742\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.736919\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.829605\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.019366\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.461153\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.694235\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.317106\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.499243\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.773002\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.723603\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.661827\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.309377\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.526974\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.596686\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.798740\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.397252\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.729217\n",
      "\n",
      "Test set: Average loss: 0.5309, Accuracy: 9004/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.470255\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.719660\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.626870\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.617200\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.572894\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.542371\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.445521\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.376469\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.530715\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.416222\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.738505\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.494627\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.381598\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.257727\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.270459\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.372318\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.469288\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.388577\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.650046\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.384576\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.376241\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.679775\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.953263\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.339054\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.468579\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.489782\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.599529\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.585101\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.555414\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.746299\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.573364\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.858287\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.590895\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.520248\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.386414\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.419864\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.370590\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.799588\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.437844\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.626135\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.561697\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.726867\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.625545\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.250574\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.840520\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.964165\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.664678\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.588771\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.414873\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.334226\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.557945\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.617007\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.945038\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.422750\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.679530\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.346208\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.461079\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.433256\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.433818\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.784177\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.523736\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.271396\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.475786\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.392557\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.541720\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.374472\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.309001\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 1.143753\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 1.022032\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.541364\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.416508\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.357385\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.891510\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.538820\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.393572\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.731349\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.377081\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.589644\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.550830\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.652258\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.445497\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.815433\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.749402\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.584083\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.641562\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.376215\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.604165\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.270425\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.492385\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.805876\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.484688\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.629149\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.552717\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.572522\n",
      "\n",
      "Test set: Average loss: 0.4943, Accuracy: 9075/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.303089\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.431780\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.455071\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.568320\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.547220\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.342773\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.425018\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.710189\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.479099\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.341467\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.507339\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.481791\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.822257\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.379382\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.807880\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.531479\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.490342\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.519527\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.632062\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.467742\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.705369\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.555536\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.626006\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.487324\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.371456\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.356136\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.397778\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.693807\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.751048\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.509973\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.940351\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.446150\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.523108\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.858666\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.508624\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.491083\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.298998\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.759065\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.613792\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.282544\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.452381\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.438118\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.520018\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.517270\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.550642\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.417164\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.420566\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.602762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.413157\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.203808\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.530979\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.363951\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.342181\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.642413\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.372060\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.369926\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.499412\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.243360\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.496434\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.769916\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.739880\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.339512\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.517998\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.672433\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.253948\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.434998\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.162737\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.213270\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.391161\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.491829\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.521207\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.345071\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.276623\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.398079\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.260232\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.467343\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.562244\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.432269\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.312688\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.306103\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.665031\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.718514\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.696654\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.497211\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.305858\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.563350\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.394738\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.396702\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.483070\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.291668\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.577146\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.760790\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.427603\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.466955\n",
      "\n",
      "Test set: Average loss: 0.4896, Accuracy: 9076/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.410555\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.220411\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.382430\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.513884\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.246524\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.575824\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.460023\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.395356\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.212176\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.432899\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.393985\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.340090\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.481104\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.464195\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.607124\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.708682\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.563369\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.405664\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.399267\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.681943\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.446946\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.479998\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.516145\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.378291\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.313139\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.271815\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.448425\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.450942\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.622274\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.376212\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.297849\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.407159\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.426608\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.346134\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.530636\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.477397\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.355082\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.265756\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.521718\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.475435\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.350749\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.407272\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.539788\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.334158\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.531266\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.435527\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.362645\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.742932\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.844929\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.443703\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.698026\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.528677\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.608637\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.843085\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.392382\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.425652\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.418694\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.356225\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.465119\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.590327\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.414575\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.365651\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.615920\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.304164\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.508607\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.256201\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.398962\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.542915\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.334029\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.615615\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.493393\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.467328\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.684827\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.619437\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.505439\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.696023\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.336075\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.413121\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.360140\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.526114\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.287017\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.701345\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.413408\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.427979\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.450609\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.315484\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.256392\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.628272\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.546175\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.178089\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.360754\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.458935\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.330661\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.589889\n",
      "\n",
      "Test set: Average loss: 0.4870, Accuracy: 9089/10000 (91%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.540939\n",
      "Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.326367\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.382046\n",
      "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.253816\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.472694\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.470603\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.562699\n",
      "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.472033\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.330708\n",
      "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.506088\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.803751\n",
      "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.637715\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.488007\n",
      "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.382799\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.644439\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.401071\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.382959\n",
      "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.481550\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.494294\n",
      "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.741302\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.370764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.406626\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.392589\n",
      "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.277136\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.792968\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.471407\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.244332\n",
      "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.473506\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.831768\n",
      "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.310001\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.205651\n",
      "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.447895\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.233634\n",
      "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.562742\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.634941\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.256565\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.689846\n",
      "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.559391\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.446611\n",
      "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.491720\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.368040\n",
      "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.490455\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.648477\n",
      "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.442360\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.570625\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.281587\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.595611\n",
      "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.392956\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.464996\n",
      "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.379891\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.327923\n",
      "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.262212\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.360175\n",
      "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.296505\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.671948\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.316008\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.372703\n",
      "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.339109\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.674088\n",
      "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.411525\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.449761\n",
      "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.389529\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.350456\n",
      "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.343763\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.385231\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.466440\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.613049\n",
      "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.541903\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.444940\n",
      "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.346011\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.293007\n",
      "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.353830\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.298250\n",
      "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.695865\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.310909\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.421221\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.515324\n",
      "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.443871\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.498464\n",
      "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.612135\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.484532\n",
      "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.355345\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.363496\n",
      "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.449763\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.241291\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.396719\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.413124\n",
      "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.481274\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.510173\n",
      "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.531962\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.433622\n",
      "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.687462\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.374660\n",
      "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.480224\n",
      "\n",
      "Test set: Average loss: 0.5086, Accuracy: 9061/10000 (91%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.510762\n",
      "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.338153\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.448499\n",
      "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.493237\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.295635\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.309333\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.685554\n",
      "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.267133\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.316395\n",
      "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.396152\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.365925\n",
      "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.382873\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.322450\n",
      "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.267719\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.975912\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.565332\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.335869\n",
      "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.682211\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.662749\n",
      "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.367554\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.568745\n",
      "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.539471\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.249523\n",
      "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.487097\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.261684\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.416475\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.392921\n",
      "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.416777\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.258144\n",
      "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.255917\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.300081\n",
      "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.436514\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.324045\n",
      "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.381810\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.518723\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.332508\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.444020\n",
      "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.450088\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.346494\n",
      "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.461003\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.689291\n",
      "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.548862\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.532893\n",
      "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.269653\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.305168\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.347247\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.296874\n",
      "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.259084\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.256555\n",
      "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.320049\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.296460\n",
      "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.253982\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.194248\n",
      "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.408385\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.385299\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.330899\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.344558\n",
      "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.381834\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.404878\n",
      "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.343648\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.574934\n",
      "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.540373\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.659135\n",
      "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.284316\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.419104\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.535775\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.312182\n",
      "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.310997\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.301419\n",
      "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.190451\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.464357\n",
      "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.473715\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.422610\n",
      "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.390578\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.396871\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.369844\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.457414\n",
      "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.667321\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.425863\n",
      "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.412371\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.384040\n",
      "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.568705\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.388388\n",
      "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.574434\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.471678\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.334117\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.359256\n",
      "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.389832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.282071\n",
      "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.435005\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.464853\n",
      "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.245965\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.454898\n",
      "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.982634\n",
      "\n",
      "Test set: Average loss: 0.4770, Accuracy: 9118/10000 (91%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.306427\n",
      "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.328410\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.272571\n",
      "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.657770\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.429852\n",
      "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.227559\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.438782\n",
      "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.144411\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.531551\n",
      "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.326070\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.664456\n",
      "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.254271\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.489301\n",
      "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.452751\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.139817\n",
      "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.692684\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.445253\n",
      "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.320805\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.450675\n",
      "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.453093\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.510446\n",
      "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.559319\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.460675\n",
      "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.290172\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.381267\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.566370\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.276639\n",
      "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.417536\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.383874\n",
      "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.533956\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.301039\n",
      "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.335619\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.581810\n",
      "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.762266\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.321002\n",
      "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.329267\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.536275\n",
      "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.413636\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.237699\n",
      "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.418257\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.852433\n",
      "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.436276\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.447458\n",
      "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.428076\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.659905\n",
      "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.451939\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.488048\n",
      "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.294046\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.526487\n",
      "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.726730\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.623224\n",
      "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.438958\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.478209\n",
      "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.272177\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.477092\n",
      "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.367310\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.464170\n",
      "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.376082\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.379960\n",
      "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.438549\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.452808\n",
      "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.865682\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.469995\n",
      "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.443012\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.561583\n",
      "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.253039\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.169542\n",
      "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.343660\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.291200\n",
      "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.308240\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.342241\n",
      "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.483223\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.337890\n",
      "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.211845\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.508132\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.303175\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.714132\n",
      "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.174351\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.581858\n",
      "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.791527\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.881967\n",
      "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.271763\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.378996\n",
      "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.608196\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.523320\n",
      "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.670985\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.399410\n",
      "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.278151\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.613608\n",
      "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.325072\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.393295\n",
      "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.315556\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.444007\n",
      "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.519433\n",
      "\n",
      "Test set: Average loss: 0.4909, Accuracy: 9140/10000 (91%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.308741\n",
      "Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.299618\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.580202\n",
      "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.083530\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.467412\n",
      "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.326002\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.283293\n",
      "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.243770\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.834885\n",
      "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.505760\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.682990\n",
      "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.297643\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.265222\n",
      "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.318613\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.598471\n",
      "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.369454\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.325945\n",
      "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.234872\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.525940\n",
      "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.323610\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.359243\n",
      "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.432826\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.470576\n",
      "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.510707\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.257203\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.387642\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.416405\n",
      "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.180441\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.242050\n",
      "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.543918\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.295921\n",
      "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.447140\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.377912\n",
      "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.421947\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.327230\n",
      "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.388237\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.553318\n",
      "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.260139\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.202084\n",
      "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.312428\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.517702\n",
      "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.134508\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.330056\n",
      "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.429891\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.448203\n",
      "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.377390\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.276508\n",
      "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.493184\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.743487\n",
      "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.343038\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.581079\n",
      "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.553779\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.457808\n",
      "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.396771\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.274670\n",
      "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.270532\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.438852\n",
      "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.492517\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.430451\n",
      "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.326039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.366971\n",
      "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.429976\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.330161\n",
      "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.424360\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.434064\n",
      "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.498141\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.319677\n",
      "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.534761\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.261612\n",
      "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.517392\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.440436\n",
      "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.254039\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.491410\n",
      "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.369566\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.340459\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.284715\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.477594\n",
      "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.507961\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.319872\n",
      "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.698200\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.476535\n",
      "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.501728\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.997327\n",
      "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.315718\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.461281\n",
      "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.464718\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.336759\n",
      "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.502657\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.510117\n",
      "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.281902\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.472030\n",
      "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.295337\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.305098\n",
      "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.385930\n",
      "\n",
      "Test set: Average loss: 0.4797, Accuracy: 9172/10000 (92%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# Notebook specific hack\n",
    "import sys; sys.argv=['']; del sys\n",
    "run_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0204, len 10000 \n",
      "\n",
      "Ref loss 0.0204180196672678\n",
      "Trained on MNIST we detected on MNIST 350/10000 (3.50%) out of distribution\n",
      "Trained on MNIST we detected on FashionMNIST 7334/10000 (73.34%) out of distribution\n",
      "AUC for MNIST : 0.9692911949999999\n",
      "\n",
      "Test set: Average loss: 0.0490, len 10000 \n",
      "\n",
      "Ref loss 0.0490430459022522\n",
      "Trained on FashionMNIST we detected on MNIST 6141/10000 (61.41%) out of distribution\n",
      "Trained on FashionMNIST we detected on FashionMNIST 1040/10000 (10.40%) out of distribution\n",
      "AUC for FashionMNIST : 0.8571711399999999\n"
     ]
    }
   ],
   "source": [
    "def test_datasets():\n",
    "    batch_size = 500\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    for dataset_cls in datasets:\n",
    "        filename = f'{dataset_cls.__name__}.pt'\n",
    "        \n",
    "        model = MultiNet(Net(), Net()).to(device)\n",
    "        model.load_state_dict(torch.load(filename))\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "                dataset_cls('../data', train=False, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "             ])), batch_size=batch_size)\n",
    "        ref_kl_loss = kl(model, device, test_loader)\n",
    "        print(\"Ref loss\", ref_kl_loss)\n",
    "        \n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        for dataset_cls2 in datasets:\n",
    "            test_loader2 = torch.utils.data.DataLoader(\n",
    "                dataset_cls2('../data', train=False, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "             ])),batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            \n",
    "            OOD = 0\n",
    "            for data, target in test_loader2:\n",
    "                outputs = model(data.to(device))\n",
    "                kl_loss = torch.max(F.kl_div(outputs[0], outputs[1].exp(), reduction='none'),  F.kl_div(outputs[1], outputs[0].exp(), reduction='none'))\n",
    "                kl_loss = kl_loss.sum(dim=-1)\n",
    "                \n",
    "                similar = outputs[0].argmax(dim=-1) == outputs[1].argmax(dim=-1)\n",
    "                \n",
    "                normed = kl_loss / ref_kl_loss\n",
    "                \n",
    "                kl_anomaly = normed > 10\n",
    "                non_concordant = similar == False\n",
    "                \n",
    "                out_of_distrib = sum(kl_anomaly | non_concordant)\n",
    "                \n",
    "                N = normed.shape[0]\n",
    "                boolean = dataset_cls2 != dataset_cls\n",
    "                all_labels.extend([boolean] * N)\n",
    "                all_scores.extend(normed.tolist())\n",
    "                \n",
    "                \n",
    "                OOD += out_of_distrib\n",
    "            print(f\"Trained on {dataset_cls.__name__} we detected on {dataset_cls2.__name__} {OOD}/{len(test_loader2.dataset)} ({float(OOD)/len(test_loader2.dataset) * 100:.2f}%) out of distribution\")\n",
    "                   \n",
    "        auc = roc_auc_score(all_labels, all_scores)\n",
    "        print(f\"AUC for {dataset_cls.__name__} : {auc}\")\n",
    "\n",
    "test_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that we achieve, with no tuning whatsoever a decent out of distribution detector. We seem to achieve much better AUROC on MNIST, probably because the in-distribution learning seems to be much better (99% test accuracy vs 92% for fastionMNIST). So to False positives for fashionMNIST probably come from this hard to learn in-distribution. Some fine tuning needs to be done to get better results. We also have to keep in mind, that the models to learn this are quite small (2M parameters but only 2 convolution layers) so the lottery hypothesis validity for such a network might be questionned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 bis\n",
    "\n",
    "Same experiment but with fine tuned, larger networks on the same datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3\n",
    "\n",
    "Check that two identical networks (same initalization) actually don't work. It's just a sanity check. We should obtain always kl_div = 0 no matter where we are in the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4\n",
    "\n",
    "Test on a larger output space, like CIFAR-100 and SVHN, to check that part of the limits are actually due to small number of output classes\n",
    "for MNIST/FashionMNIST\n",
    "Other idea is to test on Transformers. Early experiment seems to show that we can use that idea to detect different language within text with just the kl_div used as a distance.\n",
    "\n",
    "Found French book within english books dataset, AND english paragraphs *within* this french book.\n",
    "Needs some work to clean this experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5\n",
    "\n",
    "Need to test with various training schemes, regularization schemes (dropout, batchnorm, l2 penalization) and so on. We should find that the smoother in-distribution our models behave the more this method should work. Hopefully test accuracy *should* be a good smoothness proxy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6  (Unsupervised text classification) or fuse with experiment 4? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show that small network trained on a single english book enables to detect different languages\n",
    "  or different patterns of writing (old english, irish, french, or event dictionnaries)\n",
    "- The detection is super fined grained capable of detecting english within a French book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7\n",
    "\n",
    "Run this method with 2, 3, 4, and so on models. We should get exponential improved accuracy, if the random behavious for out-of-distribution for models is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pros for this method are that:\n",
    " - It's super simple to implement, and only costs a constant factor in training time.\n",
    " - You could also extend this to 3, 4 side models, and it *should* improve robustness exponentially if the random factors are correct. If we keep this number small, it will still be constant cost factor.\n",
    " - It does *not* require a perturbation model for input data, which in itself is subject to fine-tuning.\n",
    "\n",
    "\n",
    "The cons is that:\n",
    "- It does not work so well on low dimensional output spaces. \n",
    "- It seems other methods have better results than this one.\n",
    "- It only works for models that output probability distributions (hard to extend to object detection, generation and other tasks)\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot more experiments necessary to verify that the hypothesis in favor of that approach hold. Try to find ways to implement that in other tasks. How to improve out-of-distribution detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
