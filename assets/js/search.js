
var documents = [{
    "id": 0,
    "url": "https://Narsil.github.io/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "https://Narsil.github.io/about/",
    "title": "About Me",
    "body": "I’m an entrepreneur technical guy. Built french math/physics online training platform Kwyk. Interested in Machine Learning that actually works. Regularly trying to learn and try new languages/frameworks. "
    }, {
    "id": 2,
    "url": "https://Narsil.github.io/categories/",
    "title": "Tags",
    "body": "Contents: {% if site. categories. size &gt; 0 %} {% for category in site. categories %} {% capture category_name %}{{ category | first }}{% endcapture %} {{ category_name }}{% endfor %}{% endif %} {% for category in site. categories %}  {% capture category_name %}{{ category | first }}{% endcapture %} &lt;h3 id = {{ category_name }} &gt;&lt;i class= fas fa-tags category-tags-icon &gt;&lt;/i&gt;&lt;/i&gt; {{ category_name }}&lt;/h3&gt;&lt;a name= {{ category_name | slugize }} &gt;&lt;/a&gt;{% for post in site. categories[category_name] %}{%- assign date_format = site. minima. date_format | default:  %b %-d, %Y  -%}&lt;article class= archive-item &gt; &lt;p class= post-meta post-meta-title &gt;&lt;a class= page-meta  href= {{ site. baseurl }}{{ post. url }} &gt;{{post. title}}&lt;/a&gt; • {{ post. date | date: date_format }}&lt;/p&gt;&lt;/article&gt;{% endfor %} {% endfor %}"
    }, {
    "id": 3,
    "url": "https://Narsil.github.io/images/copied_from_nb/",
    "title": "",
    "body": "WarningDo not manually save images into this folder. This is used by GitHub Actions to automatically copy images.  Any images you save into this folder could be deleted at build time. "
    }, {
    "id": 4,
    "url": "https://Narsil.github.io/2020/02/26/self-kl-divergence-for-out-of-distribution-detection.html",
    "title": "Self KL-divergence for detecting out of distribution data and unsupervised text classification",
    "body": "2020/02/26 -  TL;DR. By training two models at the same time (same architecture, same loss, but different initialization)I was able to obtain a consistent out-of-distribution detector by measuring the kl-divergence between model outputs. This out-of-distribution measure used on text could lead to unsupervised text classification. 1/ What’s the problem ? ML models usually are not really capable of predicting how well the data youfeed them is close to what was in the dataset. It really matters in productionmodels as they might make really stupid mistakes just because they are offthe training set.  Find a good example of failure Find classical solutions to this problem.  Good thing that makes this measure better than other methods is that it’s measured in bits/nats so you know.    Less tl;dr as to why this should work (lottery ticket hypothesis, model structure forces some relevant manifold of the output   https://ai. googleblog. com/2019/12/improving-out-of-distribution-detection. html https://arxiv. org/pdf/1910. 04241. pdf (Manifold approximation via Embedding (VAE or GAN) and out-of-distribution sampling via Manifold perturbation. ) https://paperswithcode. com/task/out-of-distribution-detection https://openreview. net/pdf?id=Hkxzx0NtDB (Energy based model, hard to train but effective, learn p(x, y) at the same time as p(y|x)) https://arxiv. org/pdf/1802. 04865v1. pdf (Adding extra loss making optimization problem joint between confidence and accuracy)2/ How do we solve it ? Tl;dr : Make two similar models, with two different random initialization, then train them at the same time. Check their converged average kl-distance on the train set, that will give you a baseline of what similar is. Check what kind of values do you get on test/validation set. You should get something similar or higher. Then you have by measuring this self kl-divergence on new sample a measure of newness. Then it’s a matter of choosing yourown threshold about what’s acceptable or not. linked to the training data leading to good properties in terms of out of distribution values). 3/ Experiments  Test two identical networks. With same training we should have kl-divergence = 0 everywhere. So no possibility of detecting out of distribution. Test on widely different architecture and check that we don’t get correct results On same architecture, different initialization show that it can be used for out of distribution detection for english, french and the train set.  Test with various initialization patterns, with various architectures. Show that it’s linked todescent method, and probably structure of network (does not seem fully generalizable) Test with random inputs to check that it works.  Test with adversarial sampling to see if we can generate samples from it. 4/ Unsupervised text classification  Show that small network trained on a single english book enables to detect different languagesor different patterns of writing (old english, irish, french, or event dictionnaries) The detection is super fined grained capable of detecting english within a French book. 5/ Limits 6/ Future work "
    }, {
    "id": 5,
    "url": "https://Narsil.github.io/fastpages/jupyter/2020/02/20/test.html",
    "title": "Fastpages Notebook Blog Post",
    "body": "2020/02/20 -           About&#182;This notebook is a demonstration of some of capabilities of fastpages with notebooks. With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! Front Matter&#182;: Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: Setting toc: true will automatically generate a table of contentsSetting badges: true will automatically include GitHub and Google Colab links to your notebook. Setting comments: true will enable commenting on your blog post, powered by utterances. More details and options for front matter can be viewed on the front matter section of the README. Markdown Shortcuts&#182;: put a #hide flag at the top of any cell you want to completely hide in the docs put a #collapse flag at the top of any cell if you want to hide that cell by default, but stil have it be visible to the reader:              #collapseimport pandas as pdimport altair as alt       put a #collapse_show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it:              #collapse_showcars = &#39;https://vega. github. io/vega-datasets/data/cars. json&#39;movies = &#39;https://vega. github. io/vega-datasets/data/movies. json&#39;sp500 = &#39;https://vega. github. io/vega-datasets/data/sp500. csv&#39;stocks = &#39;https://vega. github. io/vega-datasets/data/stocks. csv&#39;flights = &#39;https://vega. github. io/vega-datasets/data/flights-5k. json&#39;       Interactive Charts With Altair&#182;: Charts made with Altair remain interactive.  Example charts taken from this repo, specifically this notebook. Example 1: DropDown&#182;:       # single-value selection over [Major_Genre, MPAA_Rating] pairs# use specific hard-wired values as the initial selected valuesselection = alt. selection_single(  name=&#39;Select&#39;,  fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;],  init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;},  bind={&#39;Major_Genre&#39;: alt. binding_select(options=genres), &#39;MPAA_Rating&#39;: alt. binding_radio(options=mpaa)}) # scatter plot, modify opacity based on selectionalt. Chart(movies). mark_circle(). add_selection(  selection). encode(  x=&#39;Rotten_Tomatoes_Rating:Q&#39;,  y=&#39;IMDB_Rating:Q&#39;,  tooltip=&#39;Title:N&#39;,  opacity=alt. condition(selection, alt. value(0. 75), alt. value(0. 05)))    Example 2: Tooltips&#182;:       alt. Chart(movies). mark_circle(). add_selection(  alt. selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;])). encode(  x=&#39;Rotten_Tomatoes_Rating:Q&#39;,  y=alt. Y(&#39;IMDB_Rating:Q&#39;, axis=alt. Axis(minExtent=30)), # use min extent to stabilize axis title placement  tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;]). properties(  width=600,  height=400)    Example 3: More Tooltips&#182;:       # select a point for which to provide details-on-demandlabel = alt. selection_single(  encodings=[&#39;x&#39;], # limit selection to x-axis value  on=&#39;mouseover&#39;, # select on mouseover events  nearest=True,  # select data point nearest the cursor  empty=&#39;none&#39;   # empty selection includes no data points)# define our base line chart of stock pricesbase = alt. Chart(). mark_line(). encode(  alt. X(&#39;date:T&#39;),  alt. Y(&#39;price:Q&#39;, scale=alt. Scale(type=&#39;log&#39;)),  alt. Color(&#39;symbol:N&#39;))alt. layer(  base, # base line chart    # add a rule mark to serve as a guide line  alt. Chart(). mark_rule(color=&#39;#aaa&#39;). encode(    x=&#39;date:T&#39;  ). transform_filter(label),    # add circle marks for selected time points, hide unselected points  base. mark_circle(). encode(    opacity=alt. condition(label, alt. value(1), alt. value(0))  ). add_selection(label),  # add white stroked text to provide a legible background for labels  base. mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2). encode(    text=&#39;price:Q&#39;  ). transform_filter(label),  # add text labels for stock prices  base. mark_text(align=&#39;left&#39;, dx=5, dy=-5). encode(    text=&#39;price:Q&#39;  ). transform_filter(label),    data=stocks). properties(  width=700,  height=400)    Data Tables&#182;: You can display tables per the usual way in your blog:       movies = &#39;https://vega. github. io/vega-datasets/data/movies. json&#39;df = pd. read_json(movies)# display table with pandasdf[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;,   &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]]. head()           Title   Worldwide_Gross   Production_Budget   IMDB_Rating         0   The Land Girls   146083. 0   8000000. 0   6. 1       1   First Love, Last Rites   10876. 0   300000. 0   6. 9       2   I Married a Strange Person   203134. 0   250000. 0   6. 8       3   Let's Talk About Sex   373615. 0   300000. 0   NaN       4   Slam   1087521. 0   1000000. 0   3. 4     Images&#182;: Local Images&#182;: You can reference local images and they will be copied and rendered on your blog automatically.  You can include these with the following markdown syntax: ![](my_icons/fastai_logo. png) Remote Images&#182;: Remote images can be included with the following markdown syntax: ![](https://image. flaticon. com/icons/svg/36/36686. svg) Animated Gifs&#182;: Animated Gifs work, too! ![](https://upload. wikimedia. org/wikipedia/commons/7/71/ChessPawnSpecialMoves. gif) Captions&#182;: You can include captions with markdown images like this: ![](https://www. fast. ai/images/fastai_paper/show_batch. png  Credit: https://www. fast. ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/ ) Other Elements&#182;Tweetcards&#182;: Typing &gt; twitter: https://twitter. com/jakevdp/status/1204765621767901185?s=20 will render this:Altair 4. 0 is released! https://t. co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t. co/roXmzcsT58 . . . read on for some highlights. pic. twitter. com/vWJ0ZveKbZ &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Youtube Videos&#182;: Typing &gt; youtube: https://youtu. be/XfoYk_Z5AkI will render this: Boxes / Callouts&#182;: Typing &gt; Warning: There will be no second warning! will render this:    Warning: There will be no second warning! Typing &gt; Important: Pay attention! It's important. will render this:    Important: Pay attention! It&#8217;s important. Typing &gt; Tip: This is my tip. will render this:    Tip: This is my tip. Typing &gt; Note: Take note of this. will render this:    Note: Take note of this. Typing &gt; Note: A doc link to [an example website: fast. ai](https://www. fast. ai/) should also work fine. will render in the docs:    Note: A doc link to an example website: fast. ai should also work fine. "
    }, {
    "id": 6,
    "url": "https://Narsil.github.io/ml/nlp/2019/08/06/model-based-bpe-encodings-3.html",
    "title": "Model based encodings (3)",
    "body": "2019/08/06 - In the first segmentwe looked into how we could make a BPEbased encoding, not only based on frequency in the dataset, but directly on themodel probability measure of the next token. In that article I mention thatdynamic BPE are costly because they stop being a one time operation but have tobe done for every batch because the vocabulary might have changed. In thisarticle I try to completely remove the “static” BPE approach and replace itcompletely with ML blocks.  TL;DR In this article we present an idea to replace classical BPE algorithm with a pure ML version of it. What is the goal ?: So the goal is to replace BPE algorithm. So it’s go from something like “T|h|e| |c|a|t| |a|t|e| |t|h|e| |a|p|p|l|e|. ” To something that has less elements : “The |ca|t |at|e |the| |app|le|. ” In one sentence, BPE fuses bytes to form tokens based on frequency in the fulldataset. For a more detailed example, look that the previousarticle. In this example, you can see there is always a split after a space. That’s alimitation of BPE so actually our target might look different, maybe more like “The cat |at|e |the app|le|. ” Here we can notice that “The cat” is a full token and contain 2 actual words. So the goal is to fuse some starting bytes into N tokens (let’s say ~10k) thathopefully capture regularities in our dataset and are at least correlated tofrequency in the original dataset like BPE was. Another property we need to have from BPE is that it can encode an arbitrarystring of text. It does not matter if it’s not the same language or even if itmakes sense, you CAN encode it, that is a very desirable property. It avoidsthe out-of-vocabulary problem. Approach: Tokenization: So let’s imagine we have a trained transformer likeGPT-2. But trained on bytesdirectly NOT on tokens like the original transformer. Now we can use the ideathat when a model is highly confident, it probably means that what it’s aboutto predict is “in the same token”. Let’s take an example. Try to predict thefollowing Character (as in a single letter) in the next 2 sentences  Sentence 1: “Who are yo…”  Sentence 2 : “I like …” In the first sentence, normally you would vote with very high confidence for“u”, whereas in the second sentence, you lack a lot of context to be exactlysure on what’s coming next. So “you” would be a token, whereas “like …” can’tbe a single token, it has to be at least 2, “like “ and “…”. Here is a small gif of actual probabilities of the language model on a small sentence You can see the in the left of the graph the probabilities drop, those are thetokens that try to get predicted but are missing context (because we have veryfew characters before them. For the right side, you can see the drops in probabilityare pretty consistent and correspond to word boundaries most often. Handling unknown tokens: Now we know how we are going to “fuse” characters, but we are not done yet. BPEtokens are a discrete SET of identified values from 0 to N (~10k in thisexperiment). Also BPE can encode an arbitrary new string by using it’s fusiontable. So we can’t just run our algorithm on some specific dataset, count allthe tokens created and declare that these are the N tokens for eternity. Let’simagine I feed my algorithm a new sentence, in a different language, French forinstance. “J’adore l’Italie. ” We can run our “tokenizer” on this, and receive something like this “J|’|ado|re |l’|Ita|lie. ” Now “ado” might not be in our original list, so what do we do with it ? Do wedeclare the token wrong and split it ? That would be odd. A key insight, is to remember that the first step of the discrete “token” onceit enters the model (all of them do that, it’s really not specific totransformer or GPT-2) it gets embedded, meaning we go from a number between 1and N, to a vector in d dimension space (d is between 100 and 1000 generally). For instance token 3 gets mapped to [0. 3, -0. 15, 1. 4, …] while token 4 gets mappedto [-2. 4, -0. 014, 0. 45, …] So the idea it to generate directly a token embedding (a vector in d-dimension), not necessarily adiscrete value (a number between 0 and vocabulary size). In order to do that we need that all tokens should now be represented in thesame way by a d dimension space vector. One way to achieve that is to use anautoencoder. or with code The core idea is that when we encounter a new unseen token like “ado” it will still havea representation through the VAE, and will probably be close to a known token like “add”. This can help the network overcome odd tokenization or spelling errors. ## The name is VAE but I didn't use the internal KL loss in the end as it prevented/slowed down the learning. class VAE(nn. Module):  def __init__(self):    super(VAE, self). __init__()    self. M = config. CONTEXT_SIZE * config. EMBEDDING_DIM    layer = nn. Linear    m = 400    self. fc1 = layer(self. M, m)    self. fc21 = layer(m, config. EMBEDDING_DIM)    self. fc22 = layer(m, config. EMBEDDING_DIM)    self. fc3 = layer(config. EMBEDDING_DIM, m)    self. fc4 = layer(m, self. M)  def encode(self, x):    # x is [Batch, Context size, Embedding dim]    x = x. view(-1, self. M)    h1 = F. relu(self. fc1(x))    return self. fc21(h1), self. fc22(h1)  def reparameterize(self, mu, logvar):    std = torch. exp(0. 5 * logvar)    eps = torch. randn_like(std)    return mu + eps * std  def decode(self, z):    h3 = F. relu(self. fc3(z))    return torch. tanh(      self. fc4(h3). view(-1, config. CONTEXT_SIZE, config. EMBEDDING_DIM)    )  def forward(self, x):    mu, logvar = self. encode(x)    z = self. reparameterize(mu, logvar)    return mu, logvar, z, self. decode(z)Final network: Results: Here is a summary of the values of the tokenization we got.           Raw   BPE   Model based         Vocabulary size   256   10000   26262       #Tokens   387k   90k   92k       Avg token length   1   3. 3   6. 65   Here is a excerpt of the kind of tokenization we created |He w|as on|e of|the |most |n|oticea|ble member|s of the| Reform| Club|, |th|ough| he| s|eemed|always |to |avoid |att|racting at|tention|; an en|ig|mat|i|cal |p|erson|age|,||ab|out whom l|ittle| was |known|, |e|xc|ept that |he| w|as |a |poli|shed m|an|o|f |th|e |wo|rld|.  |Pe|ople sa|id| that h|e |re|sembl|ed| |Byron|--at least|t|hat |his hea|d w|as |Byronic|; |but| he was |a |b|earde|d, tranquil| Byron|,who| |might live| on a |thousand year|s |w|ithout g|r|owing o|ld|. ||Certainly| an| English|man|, it |was |m|ore |doubt|ful w|h|ether |Phileas Fogg|w|as |a |London|er|. Full text This version has been done with epsilon=0. 0015. As you can see, “Phileas Fogg” is already a token in this situation, which is a multi-word token notachievable by regular BPE. You can also see, a lot of words contain only single bytes tokens whichis why this method compresses LESS than regular BPE at the same vocabulary size. Another note is that classical words like “was” is already a token (in the last sentence) but it’s not alwaysthe case, this token is context dependent now ! VAE: After the VAE step, the reconstruction is not perfect yet perfectly legible. |He w|as on|e of|the |most |n|oticea|ihe member|s of the| reform| Club|, |th|ough| he| s|eemed|always |to |asoid |att|nacting at|tention|, an en|ig|mat|i|cal |p|erson|age|,||ab|it whom l|ittle| was | nown|, |e|xc| pt that |he| w|as |a |poli|shed m|an|o|f |th|e |wo|rld|.  |Pe|ople sa|id| that h|e |re|sembl|ed| |pyron| cat least|t|hat |has hea|d w|as |blronic|; |but| he was |a |b|earde|in tranquil| pyron|who| |eight live| on a |dar and year|s |w|ithout g|r|owing o|ld|. ||rertainly| an| English|man|, it |was |m|ore |doubt|ful w|h|ether |Phileas Fogg|w|as |a |London|er|. Full text Most of the errors tend to lie in the first characters of long tokens. That’s because, I’m forced to paddthe input of the VAE and to mask that padding. In practice that means that the first characters of long tokens get updatedless that the others so necessarily they contain more errors. More information. Upper level: In order to complete the experiment, we need to check that the original language modeldone directly at BPE level can be done with this new model-based BPE encoding. It’s pretty slow to train that upper level because we need to flow thegradients all the way through the VAE decoder, and the lower layer decodingstep, in order to get the character level loss (softmax + nll_loss) to properly train something. That’s a limit of the current approach. If we randomly split the text into train&amp;validation, we can learn almost perfectly (97% top-1 character level accuracy)the language model on top of that Model based BPE.  However this can be considered overfitting because even though a specific inputwas never seen in the valid set, a very close one was. If instead we try to compare with a fixed split, where the last part of the bookis considered the valid set, then we get much lower result. We could achieve 25% exact character matching, and ~77%top-10 character matching on the valid set, which is the end of the book !The same results happen with BPE, even worse ! we can’t get past 13% top-1 and 25% top-10on the regular BPE. That’s understandable because the dataset is very small andthe last part of the book is different so it’s very hard to infer it from just thebeginning and no other text. Another note, is that model based BPE are not tokenizing deterministicly, thereis some variance to it, depending on the context of a particular word. This actually seems to be a good property (See this) andmight explain away the better performance of model based BPE over regular BPE. Keep in mind it’s 25% of the characters that are correct. If we looked at a discrete view of tokens we probably would have a much higher prediction rate (it’s left for future work for now). Here is a picture from the tensorboard values, P_1 is probability that thecharacter predicted is the correct one, P_10 is that it is in the top-10values.  The overfitting starts happening around the ~1M steps mark. Notes:  In the experiment we learned model by model, freezing the lower modelbefore training something on top. It’s because the batching of differentlayers occur differently. Learning the whole thing end-to-end is probably goingto need some thought. The batching is easy for the lower level, every batchneeds a tensor of shape CONTEXT_SIZE (=64) of [0-255] ints. For the VAE, weneed to have a variable length (depending on the length token) times EMBEDDING_DIM(=128). The upper level needs only tensors of size CONTEXT_SIZE *EMBEDDING_DIM yet if we want to try and end-to-end training, we have noidea how many bytes we need to generate 1 correct tensor in the upper layer. We know it’s no more than CONTEXT_SIZE² but that would be prohibitive to usethat value.  The loss NEEDS to always be the byte-level nll loss. At first I thought asimple MSE loss in the embedding space could be enough to learn the propermodels. It seems to not be the case. I could only achieve meaningful results byalways referring to the original strings and calculating the NLL Loss. Whenusing this loss, the MSE actually increases. This leads me to think thatencoding/decoding + softmax are highly anisotropic operators. Looking at thesingular values of the embedding matrix, we can see that the highest one is7. 35, the lowest one 0. 12, so there are 2 orders of magnitude between the 2. This anisotropy means that the MSE loss which considers all dimensions of theembeddding equal is actually couting way too much some irrelevant dimensions. It would be much faster and simpler if we could train directly on MSE (it wouldenable us to train without running all the decoding steps to generate theloss). So we need to add some spectral loss on the embedding on the lowerlanguage model to test that hypothesis.  The tokens have variable lengths. In order to fix this, we have to padd allsequences during learning. Because we padd, we have to mask the paddingduring training for both VAE and upper LM. Keeping track of this is prettynifty and it means gradients on rarely used places will rarely get updated. Sowe will almost surely miss some letters in our tokens. Either at the front orthe end of the token depending on how we padd the tokens. Future work:  Actually testing discretizing the tokens to compare with the regular BPE. In that direction,also comparing with a randomized tokenizer as used in SentencePieceto make sure the results are actually comparable and are indeed linked to tokenization variance.  The masking problem really seems to be a current limit of the model. Finding a workaround would be really valuable.  The fact that the NLL loss is required slows down upper layers. It would be awesome if we could smooth outthe encoding/decoding matrix so that L2 directly for VAE and the upper layer works. It probably goes against regularlanguage model embedding so not sure it’s doable.  Making the epsilon based tokenization directly after the embedding layer. This would help stack those levels hopefully learninghigher and higer representations of text leading the sentence embedding and so on.  On the same idea, another direction would be to do actual discrete tokenization to allow for the models to stack. "
    }, {
    "id": 7,
    "url": "https://Narsil.github.io/ml/nlp/2019/06/06/model-based-bpe-encodings-2.html",
    "title": "Model based encodings (2)",
    "body": "2019/06/06 - In the first segmentwe looked into how we could make a BPEbased encoding, not only based on frequency in the dataset, but directly on themodel probability measure of the next token. In that article I mention thatdynamic BPE are costly because they stop being a one time operation but have tobe done for every batch because the vocabulary might have changed. In thisarticle I try to completely remove the “static” BPE approach and replace itcompletely with ML blocks.  TL;DR In this article we present an idea to replace classical BPE algorithm with a pure ML version of it. What is the goal ?: So the goal is to replace BPE algorithm. So it’s go from something like “T|h|e| |c|a|t| |a|t|e| |t|h|e| |a|p|p|l|e|. ” To something that has less elements : “The |ca|t |at|e |the| |app|le|. ” In one sentence, BPE fuses bytes to form tokens based on frequency in the fulldataset. For a more detailed example, look that the previousarticle. In this example, you can see there is always a split after a space. That’s alimitation of BPE so actually our target might look different, maybe more like “The cat |at|e |the app|le|. ” Here we can notice that “The cat” is a full token and contain 2 actual words. So the goal is to fuse some starting bytes into N tokens (let’s say ~10k) thathopefully capture regularities in our dataset and are at least correlated tofrequency in the original dataset like BPE was. Another property we need to have from BPE is that it can encode an arbitrarystring of text. It does not matter if it’s not the same language or even if itmakes sense, you CAN encode it, that is a very desirable property. It avoidsthe out-of-vocabulary problem. Approach: Tokenization: So let’s imagine we have a trained transformer likeGPT-2. But trained on bytesdirectly NOT on tokens like the original transformer. Now we can use the ideathat when a model is highly confident, it probably means that what it’s aboutto predict is “in the same token”. Let’s take an example. Try to predict thefollowing Character (as in a single letter) in the next 2 sentences  Sentence 1: “Who are yo…”  Sentence 2 : “I like …” In the first sentence, normally you would vote with very high confidence for“u”, whereas in the second sentence, you lack a lot of context to be exactlysure on what’s coming next. So “you” would be a token, whereas “like …” can’tbe a single token, it has to be at least 2, “like “ and “…”. Here is a small gif of actual probabilities of the language model on a small sentence You can see the in the left of the graph the probabilities drop, those are thetokens that try to get predicted but are missing context (because we have veryfew characters before them. For the right side, you can see the drops in probabilityare pretty consistent and correspond to word boundaries most often. Handling unknown tokens: Now we know how we are going to “fuse” characters, but we are not done yet. BPEtokens are a discrete SET of identified values from 0 to N (~10k in thisexperiment). Also BPE can encode an arbitrary new string by using it’s fusiontable. So we can’t just run our algorithm on some specific dataset, count allthe tokens created and declare that these are the N tokens for eternity. Let’simagine I feed my algorithm a new sentence, in a different language, French forinstance. “J’adore l’Italie. ” We can run our “tokenizer” on this, and receive something like this “J|’|ado|re |l’|Ita|lie. ” Now “ado” might not be in our original list, so what do we do with it ? Do wedeclare the token wrong and split it ? That would be odd. A key insight, is to remember that the first step of the discrete “token” onceit enters the model (all of them do that, it’s really not specific totransformer or GPT-2) it gets embedded, meaning we go from a number between 1and N, to a vector in d dimension space (d is between 100 and 1000 generally). For instance token 3 gets mapped to [0. 3, -0. 15, 1. 4, …] while token 4 gets mappedto [-2. 4, -0. 014, 0. 45, …] So the idea it to generate directly a token embedding (a vector in d-dimension), not necessarily adiscrete value (a number between 0 and vocabulary size). In order to do that we need that all tokens should now be represented in thesame way by a d dimension space vector. One way to achieve that is to use anautoencoder. or with code The core idea is that when we encounter a new unseen token like “ado” it will still havea representation through the VAE, and will probably be close to a known token like “add”. This can help the network overcome odd tokenization or spelling errors. ## The name is VAE but I didn't use the internal KL loss in the end as it prevented/slowed down the learning. class VAE(nn. Module):  def __init__(self):    super(VAE, self). __init__()    self. M = config. CONTEXT_SIZE * config. EMBEDDING_DIM    layer = nn. Linear    m = 400    self. fc1 = layer(self. M, m)    self. fc21 = layer(m, config. EMBEDDING_DIM)    self. fc22 = layer(m, config. EMBEDDING_DIM)    self. fc3 = layer(config. EMBEDDING_DIM, m)    self. fc4 = layer(m, self. M)  def encode(self, x):    # x is [Batch, Context size, Embedding dim]    x = x. view(-1, self. M)    h1 = F. relu(self. fc1(x))    return self. fc21(h1), self. fc22(h1)  def reparameterize(self, mu, logvar):    std = torch. exp(0. 5 * logvar)    eps = torch. randn_like(std)    return mu + eps * std  def decode(self, z):    h3 = F. relu(self. fc3(z))    return torch. tanh(      self. fc4(h3). view(-1, config. CONTEXT_SIZE, config. EMBEDDING_DIM)    )  def forward(self, x):    mu, logvar = self. encode(x)    z = self. reparameterize(mu, logvar)    return mu, logvar, z, self. decode(z)Final network: Results: Here is a summary of the values of the tokenization we got.           Raw   BPE   Model based         Vocabulary size   256   10000   26262       #Tokens   387k   90k   92k       Avg token length   1   3. 3   6. 65   Here is a excerpt of the kind of tokenization we created |He w|as on|e of|the |most |n|oticea|ble member|s of the| Reform| Club|, |th|ough| he| s|eemed|always |to |avoid |att|racting at|tention|; an en|ig|mat|i|cal |p|erson|age|,||ab|out whom l|ittle| was |known|, |e|xc|ept that |he| w|as |a |poli|shed m|an|o|f |th|e |wo|rld|.  |Pe|ople sa|id| that h|e |re|sembl|ed| |Byron|--at least|t|hat |his hea|d w|as |Byronic|; |but| he was |a |b|earde|d, tranquil| Byron|,who| |might live| on a |thousand year|s |w|ithout g|r|owing o|ld|. ||Certainly| an| English|man|, it |was |m|ore |doubt|ful w|h|ether |Phileas Fogg|w|as |a |London|er|. Full text This version has been done with epsilon=0. 0015. As you can see, “Phileas Fogg” is already a token in this situation, which is a multi-word token notachievable by regular BPE. You can also see, a lot of words contain only single bytes tokens whichis why this method compresses LESS than regular BPE at the same vocabulary size. Another note is that classical words like “was” is already a token (in the last sentence) but it’s not alwaysthe case, this token is context dependent now ! VAE: After the VAE step, the reconstruction is not perfect yet perfectly legible. |He w|as on|e of|the |most |n|oticea|ihe member|s of the| reform| Club|, |th|ough| he| s|eemed|always |to |asoid |att|nacting at|tention|, an en|ig|mat|i|cal |p|erson|age|,||ab|it whom l|ittle| was | nown|, |e|xc| pt that |he| w|as |a |poli|shed m|an|o|f |th|e |wo|rld|.  |Pe|ople sa|id| that h|e |re|sembl|ed| |pyron| cat least|t|hat |has hea|d w|as |blronic|; |but| he was |a |b|earde|in tranquil| pyron|who| |eight live| on a |dar and year|s |w|ithout g|r|owing o|ld|. ||rertainly| an| English|man|, it |was |m|ore |doubt|ful w|h|ether |Phileas Fogg|w|as |a |London|er|. Full text Most of the errors tend to lie in the first characters of long tokens. That’s because, I’m forced to paddthe input of the VAE and to mask that padding. In practice that means that the first characters of long tokens get updatedless that the others so necessarily they contain more errors. More information. Upper level: In order to complete the experiment, we need to check that the original language modeldone directly at BPE level can be done with this new model-based BPE encoding. It’s pretty slow to train that upper level because we need to flow thegradients all the way through the VAE decoder, and the lower layer decodingstep, in order to get the character level loss (softmax + nll_loss) to properly train something. That’s a limit of the current approach. If we randomly split the text into train&amp;validation, we can learn almost perfectly (97% top-1 character level accuracy)the language model on top of that Model based BPE.  However this can be considered overfitting because even though a specific inputwas never seen in the valid set, a very close one was. If instead we try to compare with a fixed split, where the last part of the bookis considered the valid set, then we get much lower result. We could achieve 25% exact character matching, and ~77%top-10 character matching on the valid set, which is the end of the book !The same results happen with BPE, even worse ! we can’t get past 13% top-1 and 25% top-10on the regular BPE. That’s understandable because the dataset is very small andthe last part of the book is different so it’s very hard to infer it from just thebeginning and no other text. Another note, is that model based BPE are not tokenizing deterministicly, thereis some variance to it, depending on the context of a particular word. This actually seems to be a good property (See this) andmight explain away the better performance of model based BPE over regular BPE. Keep in mind it’s 25% of the characters that are correct. If we looked at a discrete view of tokens we probably would have a much higher prediction rate (it’s left for future work for now). Here is a picture from the tensorboard values, P_1 is probability that thecharacter predicted is the correct one, P_10 is that it is in the top-10values.  The overfitting starts happening around the ~1M steps mark. Notes:  In the experiment we learned model by model, freezing the lower modelbefore training something on top. It’s because the batching of differentlayers occur differently. Learning the whole thing end-to-end is probably goingto need some thought. The batching is easy for the lower level, every batchneeds a tensor of shape CONTEXT_SIZE (=64) of [0-255] ints. For the VAE, weneed to have a variable length (depending on the length token) times EMBEDDING_DIM(=128). The upper level needs only tensors of size CONTEXT_SIZE *EMBEDDING_DIM yet if we want to try and end-to-end training, we have noidea how many bytes we need to generate 1 correct tensor in the upper layer. We know it’s no more than CONTEXT_SIZE² but that would be prohibitive to usethat value.  The loss NEEDS to always be the byte-level nll loss. At first I thought asimple MSE loss in the embedding space could be enough to learn the propermodels. It seems to not be the case. I could only achieve meaningful results byalways referring to the original strings and calculating the NLL Loss. Whenusing this loss, the MSE actually increases. This leads me to think thatencoding/decoding + softmax are highly anisotropic operators. Looking at thesingular values of the embedding matrix, we can see that the highest one is7. 35, the lowest one 0. 12, so there are 2 orders of magnitude between the 2. This anisotropy means that the MSE loss which considers all dimensions of theembeddding equal is actually couting way too much some irrelevant dimensions. It would be much faster and simpler if we could train directly on MSE (it wouldenable us to train without running all the decoding steps to generate theloss). So we need to add some spectral loss on the embedding on the lowerlanguage model to test that hypothesis.  The tokens have variable lengths. In order to fix this, we have to padd allsequences during learning. Because we padd, we have to mask the paddingduring training for both VAE and upper LM. Keeping track of this is prettynifty and it means gradients on rarely used places will rarely get updated. Sowe will almost surely miss some letters in our tokens. Either at the front orthe end of the token depending on how we padd the tokens. Future work:  Actually testing discretizing the tokens to compare with the regular BPE. In that direction,also comparing with a randomized tokenizer as used in SentencePieceto make sure the results are actually comparable and are indeed linked to tokenization variance.  The masking problem really seems to be a current limit of the model. Finding a workaround would be really valuable.  The fact that the NLL loss is required slows down upper layers. It would be awesome if we could smooth outthe encoding/decoding matrix so that L2 directly for VAE and the upper layer works. It probably goes against regularlanguage model embedding so not sure it’s doable.  Making the epsilon based tokenization directly after the embedding layer. This would help stack those levels hopefully learninghigher and higer representations of text leading the sentence embedding and so on.  On the same idea, another direction would be to do actual discrete tokenization to allow for the models to stack. "
    }, {
    "id": 8,
    "url": "https://Narsil.github.io/ml/nlp/2019/05/16/model-based-bpe-encodings.html",
    "title": "Model based encodings",
    "body": "2019/05/16 - Byte-pair encodings (BPE) are now very commonly used in NLP. In GPT-2, Byte-pair encodings are used to preformat the raw texts before feeding the model. But this is a relatively costly step for your preprocessing and has some limitations. For instance, you have to split your data on spaces if you want your byte pair algorithm to compute in reasonable time.  TL;DR In this article we present an idea to generate Byte pair encodings, not based on frequency in the dataset, but on the quality of the prediction of our model. This enables us to predict multi word tokens like “New York” and address languages that don’t use spaces to split words. What are Byte Pair Encodings ?: Byte-pair encodings are a way to compress information from pairs of bytes that will form tokens. Let’s take an example : “I love carrots and I love apples. ” This sentence read by a computer is only a sequence of bytes (bytes are simply a number between 0 and 255). That means to a computer our sentence looks like “I love carrots and I love apples. ” -&gt; [73, 32, 108, 111, 118, 101, 32, 99, 97, 114, 114, 111, 116, 115, 32, 97, 110, 100, 32, 73, 32, 108, 111, 118, 101, 32, 97, 112, 112, 108, 101, 115, 46] From that example, you may remark that some bytes are occurring multiple times together like [108, 111] that occurs twice (it’s “lo” from “love”). So let’s build a new token for this frequent pair. Numbers from 0 to 255 are already taken so we’ll take the next available number which is 256, and we are going to store that information in a table [108, 111] -&gt; 256 Now if we use that new token to encode our original bytes, whenever we encounter [108, 111], we’ll replace that by 256, so the original byte string becomes : [73, 32, 108, 256, 101, 32, 99, 97, 114, 114, 111, 116, 115, 32, 97, 110, 100, 32, 73, 32, 256, 118, 101, 32, 97, 112, 112, 108, 101, 115, 46] We went from 33 numbers to 31 numbers. We can rinse and repeat to compress the number of numbers even further. Originally, BPE was proposed as a compression algorithm. It’s not the best compression tool, so we won’t look at that side of the algorithm. Now you get what we are looking at when we train a model on BPEs, just a list of numbers. Typically a BPE vocabulary contains ~10k tokens (GPT-2 has 50k), that means it can capture very frequent words like “the” entirely, and parts of words that contain many variations like “ment” (mentally, environment …). What’s great about it it that you can now have words share semantic parts of them for their representation in your model so (environ-ment, environ-ment-al, environ-ment-ally will all share “environ” which will contain most of the semantic meaning, the rest will contain grammar information hopefully). The real advantage of BPE over classical Word Embeddings is that it does not fall into the out-of-vocabulary error (when a word was not seen). At worse you can always fall back to single bytes. What’s the problem with BPE ?: BPE algorithm is pretty bad in terms of complexity to calculate (roughly O(n²), you can look at a very good implementation https://github. com/glample/fastBPE). BPE is also pretty bad when you want to encode some new text. A greedy algorithm will be O(n) but not the best encoding possible, the best encoding possible is actually O(n²) in the general case. To be honest, most implementations split on spaces as mentioned earlier which speeds up the algorithm quite a bit. Once we have encoded a full word like “the” there is no way to add tokens to it, so it’s not necessary to look at it anymore for potential byte pairs, so we can assume the encoding&amp;table creation go from O(n²) to something much closer to O(n). In addition, at encoding time, once we know the encoding for “the” we can cache that information leading to further speed ups. But using spaces as a special character has drawbacks, namely:    We can’t address as well languages that don’t use a space to separate words like Chinese (arguably German).     We can’t encode frequently occurring multi words like “New York” or “European Union” or “black holes”  The second problem is especially bad when you consider examples where semantic is very different from the composing words like “Chicago Bulls” have nothing to do with bulls. ε-BPE or model based BPE encoding: The core idea is that instead of using frequency in the dataset to create the byte pairs, we can use the probability transition of the model to create the BPE. Let’s use some kind of transformer, GPT-2 for instance. The core idea of that model, is to predict the next token (in the BPE sense) given a fixed context size. But we can use the output probability of the model in order to create new tokens, not because they are frequent but because they are easy to predict. For instance in a book that contains a character “Sir Francis” that appears rarely, but there is only one character named “Sir …”, the algorithm might learn quite easily that “Sir “ is followed by “Francis” with great confidence, even if the occurence of the words is pretty low compared to common words like “the”, “like” and “I”. So the core algorithm, will train a simple transformer on a dataset on regular bytes (at least at the start). Then, as the algorithm learns, some predictions will be above 1-ε. We can keep track of those and keep track of the last token we received, to check if we were correct. Let’s keep a hit map to see how successful our algorithm is. For instance, I predicted “Fo” will be followed by “gg” (Phileas Fogg is a character in Around the world in 80 days) with probability &gt; 1-ε. I was correct in 14 cases, and got it wrong in 1 case (let’s say it was classical “Fo” “g “). We were correct 14/15 times that’s 93% accuracy. If we look at the fluctuation interval associated with that, we get [92. 74-93. 25%] range. If 92. 74 &gt; 1–ε we can conclude that our transition prediction is really very good, it’s not a fluke of the model. More generally, if we want 95% confidence when we upgrade this transition, we need to respect the following inequality : k / n - 1/sqrt(n) &gt; 1-ε, where k is the number of successful predictions, n is the total number of predictions and ε the probability margin explained earlier. This model is slightly different from byte pair encoding, but now we don’t suffer from the 2 problems mentioned above, we can get pretty long tokens if the dataset allows for it, and we can use Chinese or German as the space character does not play any special role. Results: Implementation can be found here. On the first run, we ran on a book Around the world in 80 days by Jules Verne. It’s a very small dataset but the idea is to check that we can actually overcome BPE’s limitations. Here are a few telling tokens that were created while running on the dataset :       Promotion #   Token created         338   “Mr. Fogg”       357   “Phileas Fogg”       360   “Passepartout”       635   “ir Franc” (Sir Francis)       781   “It was”       900   ’” asked’ (contains a quote character)   What is interesting, it that:    We managed to create multi word tokens like “Phileas Fogg”     Multi word tokens are a minority in terms of tokens created by the algorithm. Out of 421 tokens that contain a space character only 27 are multi word tokens like “New York”. The remaining 394 tokens contain an ending space, meaning our algorithm is learning word boundaries. It is reassuring because traditional BPE are usually hardcoding that information.     Multi word tokens are name of characters in the book, which are occurring frequently, they are an entity by themselves (Fogg even has 2 tokens associated to him)     2 Multi word tokens are not specific to the book, “it was” is a pretty common 2 word token in English in descriptions, “(…) asked” is a very common continuation when we start a quote and end a sentence with a question mark. We can guess that “(…) said” would be a token further down the line, but it’s harder as there are probably a wider variety of verbs that can fit (said, replied, answered and so on…)  Here is a more complete comparison of standard BPE with ε-BPE, with the first 100 tokens generated, as you can see more tokens are dedicated to syntax in eBPE, which Standard BPE ignore gladly by splitting on newlines and spaces.       Standard BPE   eBPE         ‘th’   ‘\r\n’       ‘the ‘   ’, ‘       ‘an’   ‘d ‘       ‘in’   ‘Th’       ‘ou’   ‘ve’       ‘er’   ‘y ‘       ‘ed ‘   ’; ‘       ‘ar’   ‘f ‘       ‘hi’   ’,\r\n’       ‘on’   ‘\r\n\r\n’       ‘re’   ‘th’       ‘en’   ‘qu’       ‘and ‘   ‘the’       ‘of ‘   ’ ‘       ‘st’   ‘the ‘       ‘to ‘   ‘The’       ‘as ‘   ‘\r\n’       ‘se’   ’, ‘       ‘ha’   ‘y ‘       ‘or’   ‘d ‘       ’. \r ‘   ‘Th’       ‘it’   ‘ve’       ‘he ‘   ’; ‘       ‘le’   ‘f ‘       ‘ing ‘   ’,\r\n’       ’,\r ‘   ’ ‘       ‘as’   ‘\r\n’       ‘in ‘   ’, ‘       ‘at’   ‘d ‘       ‘at ‘   ‘y ‘       ‘ro’   ‘Th’       ‘er ‘   ‘ve’       ‘al’   ‘f ‘       ‘es’   ’; ‘       ‘on ‘   ’ ‘       ‘was ‘   ’,\r\n’       ‘no’   ‘th’       ‘his ‘   ‘\r\n’       ‘ed’   ’, ‘       ‘ac’   ‘d ‘       ’“\r ‘   ‘y ‘       ‘ri’   ‘Th’       ‘be’   ‘ve’       ‘ly ‘   ‘f ‘       ‘om’   ’; ‘       ‘li’   ’ ‘       ‘en ‘   ’,\r\n’       ‘ti’   ‘th’       ‘og’   ‘\r\n\r\n’       ‘ra’   ‘the’       ‘di’   ‘the ‘       ‘art’   ‘The’       ‘Fog’   ‘qu’       ‘the’   ’s ‘       ‘ma’   ‘The ‘       ‘ve ‘   ‘g ‘       ‘is ‘   ’,”’       ‘or ‘   ‘no’       ‘ld ‘   ‘t ‘       ‘whi’   ‘th ‘       ‘il’   ‘o ‘       ‘ur’   ’?”’       ’s, ‘   ‘\r\n\r\n”’       ‘de’   ’,” ‘       ‘wh’   ‘Mr’       ‘lo’   ‘e ‘       ‘ch ‘   ‘yo’       ‘ere ‘   ‘Yo’       ‘ith ‘   ‘ou’       ‘The ‘   ’. ‘       ‘am’   ‘nd ‘       ‘ent’   ‘h ‘       ‘un’   ‘n ‘       ‘gh’   ’;\r\n’       ‘with ‘   ‘og’       ‘an ‘   ‘you’       ‘oun’   ‘r ‘       ‘part’   ‘of ‘       ‘ver’   ‘to ‘       ‘si’   ’s F’       ‘had ‘   ‘Pa’       ‘not ‘   ‘as ‘       ‘ould ‘   '’s ‘       ‘ing’   ’. F’       ‘out ‘   ‘is ‘       ‘el’   ‘ld ‘       ‘sa’   ‘ng ‘       ‘ce’   ‘at ‘       ‘that ‘   ‘re’       ‘asse’   ‘ve ‘       ‘fi’   ‘gh’       ‘ol’   ‘ut ‘       ‘sh’   ‘ll’       ‘r. ‘   ‘Pas’       ’. ”\r ‘   ‘re ‘       ‘Passe’   ‘ed ‘       ‘Passepart’   ’. Fog’       ‘ut ‘   ‘ch ‘       ‘which ‘   ‘and ‘       ‘ay’   ‘ea’   I would love to check the tokenization of German or Chinese but I’m not a speaker of either language so it’s hard for me to analyze the results anyway. What’s for sure is that the technique is applicable. I also tried the technique on different types of files like wav files or mp3 files, even jpeg images. Analysis is harder to do. Still some interesting notes, it took longer for the model to emit new tokens on the mp3 files than on the wav files. The mp3 file is encoded, therefore should have a lower entropy (meaning it’s harder to predict the next token) than the wav files so the model takes longer to actually get good at predicting. It’s probable (I haven’t checked) that we have to overfit the mp3 file and jpeg files before we can predict any meaningful content (except maybe the header part) Future Work: Many interesting ideas are still left to explore to continue exploring the idea of models creating their own tokenization. For now a limiting factor is the actual BPE encoding process that takes longer and longer as the model creates new tokens. That’s because the encoding process is done in Python, so it’s quite slow and can’t be precalculated as you would do with fixed BPE encodings. To give a sense of the slowdown, the training loop starts at ~11it/s on a GTX970 and finished at roughly 10s/it. That’s a 100x slowdown over the course of the training, with only 1k tokens in the end, far from the 50k used by GPT-2 for instance. It’s going to be an actual requirement to train on larger and more representative datasets. Training on bigger datasets would help us understand how important are those multi word tokens and maybe what are those multi words. The token “(…) asked” was pretty surprising to me, I’m eager to see what else can be discovered. The actual epsilon used was 40% which actually quite a big (value was chosen with trial and error, to get a small but not null rejection rate of new tokens, to add tokens as fast as possible but not making too many mistakes). That value probably has a sweet spot depending on the number of current tokens, after speeding up the process it would be interesting to look at the best value for epsilon as a function of the number of tokens. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')
    this.metadataWhitelist = ['position']

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}